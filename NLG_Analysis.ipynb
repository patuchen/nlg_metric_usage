{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwgwMiUccT8g"
      },
      "source": [
        "## Authentication\n",
        "Import the necessary libraries and authenticate the user to access the Google Sheet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6KKSyKqryKJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QMHtP1Qkob4"
      },
      "source": [
        "## Code availability analysis\n",
        "\n",
        "Added BEFORE the original analysis so original analysis may overwrite code, variables but not the other way"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading repository information from github  \n",
        "**!! Credentials Required !!**"
      ],
      "metadata": {
        "id": "PRbP9-itfuIJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYRYMS9IB1-z"
      },
      "outputs": [],
      "source": [
        "# WORKS ONLY IN Google Colab\n",
        "\n",
        "# Sheet names to analyze\n",
        "INLG_2023_PAPERS = \"INLG 2023 Papers\"\n",
        "GENERATION = \"Generation\"\n",
        "CODE_SHEETS = [INLG_2023_PAPERS, GENERATION]\n",
        "# CODE_SHEETS = [\"INLG 2023 Papers\"]\n",
        "\n",
        "# List the column names as constants to prevent typos\n",
        "TITLE = \"Title\"\n",
        "ACL_ID = \"ACL ID\"\n",
        "PAPER_TYPE = \"Paper Type\"\n",
        "PAPER_LINK = \"Paper Link\"\n",
        "CHECKED_BY = \"Checked By\"\n",
        "COMPLETE = \"Complete\"\n",
        "LINK_TO_CODE = \"Link to Code\"  #github, some other url, \"None first view\"\n",
        "NONE_FIRST_VIEW = \"None first view\"  # value of LINK_TO_CODE if no code was found\n",
        "PROMISED_DELIVERED = \"Promised Delivered\"\n",
        "INSTALLATION = \"Installation\"\n",
        "EXPERIMENTS_COVERED = \"Experiments Covered\"\n",
        "SCRIPTS_DOCUMENTATION = \"Scripts Documentation\"\n",
        "BEST_PRACTICES = \"Best Practices\"\n",
        "ACADEMIA_INDUSTRY = \"Academia/Industry\"\n",
        "STARS = \"stars\"\n",
        "Citations = \"Citations\"\n",
        "\n",
        "# TODO: put your github credentials here\n",
        "gh_username = \"\"\n",
        "secret_token = \"\"\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sheet2df(sheet):\n",
        "  worksheet = gc.open_by_url('https://docs.google.com/spreadsheets/d/10Yvxn7sb78cZmpmM0RYoAuVHoaP_KiDc970vg9pSa2c/').worksheet(sheet)\n",
        "\n",
        "  # get_all_values gives a list of rows.\n",
        "  rows = worksheet.get_all_values()\n",
        "\n",
        "  # Convert to a DataFrame and render.\n",
        "  df = pd.DataFrame.from_records(rows[1:], columns=rows[0])\n",
        "  return df\n",
        "\n",
        "codedfs = {sheet: sheet2df(sheet) for sheet in CODE_SHEETS}\n",
        "\n",
        "# RUN THE CELL ABOVE AND save the files or load them if you are on google colab or local\n",
        "# from google.colab import files\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def get_github_stars(url):\n",
        "    try:\n",
        "        # Parse the URL to get the path\n",
        "        path = urlparse(url).path\n",
        "        # Split the path to get the repository owner and name\n",
        "        owner, repo = path.strip(\"/\").split(\"/\")[0:2]\n",
        "        # Make a GET request to the GitHub API\n",
        "        response = requests.get(f\"https://api.github.com/repos/{owner}/{repo}\", auth=(gh_username, secret_token))\n",
        "        # Extract the number of stars from the response\n",
        "        stars = response.json()[\"stargazers_count\"]\n",
        "    except Exception as e:\n",
        "        print(f\"{url=} {owner=} {repo=}\\n{response.json()=}\\n{e}\", flush=True)\n",
        "        raise e\n",
        "    return stars\n",
        "\n",
        "if codedfs is not None:  # Hack to detect that we are in google colab where the gsheet api works\n",
        "    from google.colab import files\n",
        "\n",
        "    for sheet in codedfs:\n",
        "        df = codedfs[sheet]\n",
        "        df[PROMISED_DELIVERED] = df[PROMISED_DELIVERED].astype(str)\n",
        "        df['stars'] = df[~df[PROMISED_DELIVERED].str.contains('404') & df[LINK_TO_CODE].str.contains('github')][LINK_TO_CODE].apply(get_github_stars)\n",
        "\n",
        "        ##### HERE YOU CAN SAVE THE DATA AND LOAD IT AGAIN\n",
        "        # codedfs[sheet].to_csv(f\"{sheet}.csv\")\n",
        "        # files.download(f\"{sheet}.csv\")\n",
        "else:\n",
        "    #  assuming you have downloaded the files loads them\n",
        "    codedfs = {}\n",
        "    for sheet in CODE_SHEETS:\n",
        "        codedfs[sheet] = pd.read_csv(f\"{sheet}.csv\")\n",
        "\n",
        "# codedfs[INLG_2023_PAPERS].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of stars"
      ],
      "metadata": {
        "id": "TcvfEOUcf-3U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hwL9BK_GjiC"
      },
      "outputs": [],
      "source": [
        "# Create the histograms\n",
        "inlgcdf = codedfs[INLG_2023_PAPERS]\n",
        "aclcdf = codedfs[GENERATION]\n",
        "\n",
        "from numpy import log10\n",
        "\n",
        "logone = lambda x: log10(x+1)\n",
        "inv_logone = lambda x: 10**x - 1\n",
        "\n",
        "\n",
        "print(inlgcdf[inlgcdf[\"Paper Type\"] == \"Long\"][\"stars\"].mean())\n",
        "print(inlgcdf[inlgcdf[\"Paper Type\"] == \"Long\"][\"stars\"].median())\n",
        "print(inlgcdf[inlgcdf[\"Paper Type\"] == \"Short\"][\"stars\"].mean())\n",
        "print(inlgcdf[inlgcdf[\"Paper Type\"] == \"Short\"][\"stars\"].median())\n",
        "plt.hist(inlgcdf[inlgcdf[\"Paper Type\"] == \"Long\"][\"stars\"].apply(logone), bins=10, alpha=0.5, label='INLG Long Papers', color=\"#b8cdab\")\n",
        "plt.hist(inlgcdf[inlgcdf[\"Paper Type\"] == \"Short\"][\"stars\"].apply(logone), bins=10, alpha=0.5, label='INLG Short Papers', color=\"#e5c185\")\n",
        "locs, _ = plt.xticks()\n",
        "plt.xticks(locs, [str(int(inv_logone(loc))) for loc in locs])\n",
        "plt.xlabel(\"GitHub Stars [log scale]\")\n",
        "plt.ylabel(\"# GitHub Repos in Bin\")\n",
        "plt.legend(loc='upper right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU-L7_nKHjxj"
      },
      "outputs": [],
      "source": [
        "print(aclcdf[aclcdf[\"Paper Type\"] == \"Long\"][\"stars\"].mean())\n",
        "print(aclcdf[aclcdf[\"Paper Type\"] == \"Long\"][\"stars\"].median())\n",
        "print(aclcdf[aclcdf[\"Paper Type\"] == \"Short\"][\"stars\"].mean())\n",
        "print(aclcdf[aclcdf[\"Paper Type\"] == \"Short\"][\"stars\"].median())\n",
        "\n",
        "plt.hist(aclcdf[aclcdf[\"Paper Type\"] == \"Long\"][\"stars\"].apply(logone), bins=10, alpha=0.5, label='ACL Long Papers', color=\"#b8cdab\")\n",
        "plt.hist(aclcdf[aclcdf[\"Paper Type\"] == \"Short\"][\"stars\"].apply(logone), bins=10, alpha=0.5, label='ACL Short Papers', color=\"#e5c185\")\n",
        "locs, _ = plt.xticks()\n",
        "plt.xticks(locs, [str(int(inv_logone(loc))) for loc in locs])\n",
        "plt.xlabel(\"GitHub Stars [log scale]\")\n",
        "plt.ylabel(\"# GitHub Repos in Bin\")\n",
        "plt.legend(loc='upper right', title=\"GitHub Stars\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_zfp7AXNMrH"
      },
      "outputs": [],
      "source": [
        "acl_stars_mean = aclcdf[\"stars\"].mean()\n",
        "inlg_stars_mean = inlgcdf[\"stars\"].mean()\n",
        "print(f\"mean acl {acl_stars_mean:.2f} vs ingl {inlg_stars_mean:.2f}\")\n",
        "acl_stars_q50 = aclcdf[\"stars\"].median()\n",
        "inlg_stars_q50 = inlgcdf[\"stars\"].median()\n",
        "print(f\"q50 acl {acl_stars_q50:.2f} vs ingl {inlg_stars_q50:.2f}\")\n",
        "\n",
        "\n",
        "aclcdf[\"stars\"].apply(logone).hist(bins=10, alpha=0.8, label=\"ACL\", color=\"#b8cdab\")\n",
        "inlgcdf[\"stars\"].apply(logone).hist(bins=10, alpha=0.8, label=\"INLG\", color=\"#e5c185\")\n",
        "locs, _ = plt.xticks()\n",
        "plt.xticks(locs, [str(int(inv_logone(loc))) for loc in locs])\n",
        "plt.xlabel(\"GitHub Stars [log scale]\")\n",
        "plt.ylabel(\"# GitHub Repos in Bin\")\n",
        "plt.legend(loc='upper right', title=\"GitHub Stars for papers\")\n",
        "print(\"fig:stars_acl_inlg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3R_-YwLS8gt"
      },
      "outputs": [],
      "source": [
        "sheet = GENERATION\n",
        "aidf = codedfs[sheet]\n",
        "\n",
        "acl_mispromised_industry_mean = aidf[(aidf[ACADEMIA_INDUSTRY] == \"Academia\") & aidf[PROMISED_DELIVERED].str.startswith(\"NOT FILLED\")][\"stars\"].mean()\n",
        "acl_mispromised_industry_q50 = aidf[(aidf[ACADEMIA_INDUSTRY] == \"Academia\") & aidf[PROMISED_DELIVERED].str.startswith(\"NOT FILLED\")][\"stars\"].median()\n",
        "\n",
        "print(f\"{acl_mispromised_industry_mean:.2f} {acl_mispromised_industry_q50:.2f}\")\n",
        "\n",
        "aidf[(aidf[LINK_TO_CODE] != NONE_FIRST_VIEW) & (~aidf[PROMISED_DELIVERED].str.startswith(\"NOT FILLED\"))][\"stars\"].apply(logone).hist(bins=10, alpha=0.5, label=\"Delivered\", color=\"#779af5\")\n",
        "aidf[(aidf[ACADEMIA_INDUSTRY] == \"Academia\") & aidf[PROMISED_DELIVERED].str.startswith(\"NOT FILLED\")][\"stars\"].apply(logone).hist(bins=10, alpha=0.8, label=\"Missing in Academia\", color=\"#b8cdab\")\n",
        "aidf[(aidf[ACADEMIA_INDUSTRY] == \"Industry\") & aidf[PROMISED_DELIVERED].str.startswith(\"NOT FILLED\")][\"stars\"].apply(logone).hist(bins=10, alpha=0.8, label=\"Missing in Industry\", color='#e5c185')\n",
        "plt.legend(loc='upper right', title=\"ACL GitHub Stars\" if sheet == GENERATION else \"INLG GitHub Stars\")\n",
        "locs, _ = plt.xticks()\n",
        "plt.xticks(locs, [str(int(inv_logone(loc))) for loc in locs])\n",
        "plt.xlabel(\"GitHub Stars [log scale]\")\n",
        "plt.ylabel(\"# GitHub Repos in Bin\")\n",
        "\n",
        "print(\"fig:stars_mispromised\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awmke3tpfgMo"
      },
      "outputs": [],
      "source": [
        "sheet = INLG_2023_PAPERS\n",
        "aidf = codedfs[sheet]\n",
        "aidf[(aidf[LINK_TO_CODE] != NONE_FIRST_VIEW) & (~aidf[PROMISED_DELIVERED].str.startswith(\"NOT FILLED\"))][\"stars\"].apply(logone).hist(bins=10, alpha=0.5, label=\"Delivered\", color=\"#779af5\")\n",
        "aidf[(aidf[ACADEMIA_INDUSTRY] == \"Academia\") & aidf[PROMISED_DELIVERED].str.startswith(\"NOT FILLED\")][\"stars\"].apply(logone).hist(bins=10, alpha=0.8, label=\"Missing in Academia\", color=\"#b8cdab\")\n",
        "aidf[(aidf[ACADEMIA_INDUSTRY] == \"Industry\") & aidf[PROMISED_DELIVERED].str.startswith(\"NOT FILLED\")][\"stars\"].apply(logone).hist(bins=10, alpha=0.8, label=\"Missing in Industry\", color='#e5c185')\n",
        "plt.legend(loc='upper right', title=\"ACL GitHub Stars\" if sheet == GENERATION else \"INLG GitHub Stars\")\n",
        "locs, _ = plt.xticks()\n",
        "plt.xticks(locs, [str(int(inv_logone(loc))) for loc in locs])\n",
        "plt.xlabel(\"GitHub Stars [log scale]\")\n",
        "plt.ylabel(\"# GitHub Repos in Bin\")\n",
        "print(\"fig:stars_mispromised\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUDzHtn3getY"
      },
      "outputs": [],
      "source": [
        "sheet = INLG_2023_PAPERS\n",
        "aidf = codedfs[sheet]\n",
        "aidf[(aidf[INSTALLATION] == \"None\")& (aidf[LINK_TO_CODE] != NONE_FIRST_VIEW) & (~aidf[PROMISED_DELIVERED].str.startswith(\"NOT FILLED\"))][\"stars\"].apply(logone).hist(bins=10, alpha=0.2, label=\"None Instruction\", color=\"grey\")\n",
        "aidf[(aidf[INSTALLATION] == \"Basic\")& (aidf[LINK_TO_CODE] != NONE_FIRST_VIEW) & (~aidf[PROMISED_DELIVERED].str.startswith(\"NOT FILLED\"))][\"stars\"].apply(logone).hist(bins=10, alpha=0.2, label=\"Basic Instruction\", color=\"red\")\n",
        "aidf[(aidf[INSTALLATION] == \"Detailed\")& (aidf[LINK_TO_CODE] != NONE_FIRST_VIEW) & (~aidf[PROMISED_DELIVERED].str.startswith(\"NOT FILLED\"))][\"stars\"].apply(logone).hist(bins=10, alpha=0.2, label=\"Detailed Instruction\", color=\"green\")\n",
        "plt.legend(loc='upper right', title=\"ACL GitHub Stars\" if sheet == GENERATION else \"INLG GitHub Stars\")\n",
        "locs, _ = plt.xticks()\n",
        "plt.xticks(locs, [str(int(inv_logone(loc))) for loc in locs])\n",
        "plt.xlabel(\"GitHub Stars [log scale]\")\n",
        "plt.ylabel(\"# GitHub Repos in Bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of code availability and quality"
      ],
      "metadata": {
        "id": "6_pd99V9jR5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {\"ACL\": \"Generation\", \"INLG\":  \"INLG 2023 Papers\"}\n",
        "inlg = codedfs[mapping[\"INLG\"]]\n",
        "inlg_missing = inlg[inlg[PROMISED_DELIVERED].str.startswith(\"NOT FILLED\")][ACADEMIA_INDUSTRY].value_counts()\n",
        "\n",
        "\n",
        "acl = codedfs[mapping[\"ACL\"]]\n",
        "acl_missing = acl[acl[PROMISED_DELIVERED].str.startswith(\"NOT FILLED\")][ACADEMIA_INDUSTRY].value_counts()\n",
        "\n",
        "\n",
        "comparison = pd.DataFrame({'INLG 2023': inlg_missing, 'ACL 2023': acl_missing}).fillna(0).T\n",
        "ax = comparison.plot(kind='bar', stacked=True, ylim=(0,16), color=[\"#e5c185\", \"#b8cdab\", \"#779af5\"], figsize=(5,5))\n",
        "\n",
        "labels = []\n",
        "\n",
        "plt.ylabel('# of Papers')\n",
        "\n",
        "\n",
        "plt.legend(title='Institution Type', loc='upper left', ncols=1)\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.tight_layout()\n",
        "\n",
        "totals = comparison.sum(axis=1)\n",
        "\n",
        "# Annotate the bars with the percentage values\n",
        "for p in ax.patches:\n",
        "    width, height = p.get_width(), p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "\n",
        "    # Get the conference corresponding to this bar\n",
        "    conference = ax.get_xticklabels()[int(x + width / 2)].get_text()\n",
        "\n",
        "    # Calculate the percentage relative to the total for that conference\n",
        "    total = totals[conference]\n",
        "    percentage = (height / total) * 100 if total > 0 else 0\n",
        "\n",
        "    # Display the percentage\n",
        "    if height > 0:\n",
        "        ax.text(x + width / 2, y + height / 2, f'{percentage:.1f}%', ha='center', va='center')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_pnzzJ9b-9jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sca2BmWyDOQk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "counts = []  # accumulator for preprocess raw data LATER USED FOR PLOTTING GRAPHS format is tuple (Column Investigated, Conference name, Label, Label_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1aRQHGgCrN6"
      },
      "outputs": [],
      "source": [
        "label = \"Open Source?\"\n",
        "\n",
        "def missing(comment):\n",
        "    return \"Missing\" if comment.startswith('NOT FILLED') else \"Delivered\"\n",
        "\n",
        "for sheet in codedfs:\n",
        "    df = codedfs[sheet]\n",
        "    df[PROMISED_DELIVERED] = df[PROMISED_DELIVERED].fillna('None')\n",
        "    df['Category'] = df[PROMISED_DELIVERED].apply(missing)\n",
        "    missing_series = df['Category'].value_counts()\n",
        "\n",
        "    dset = \"ACL\" if sheet == GENERATION else \"INLG\"\n",
        "    for k, v in missing_series.items():\n",
        "        counts.append((label, dset, k, v))\n",
        "\n",
        "\n",
        "def classify_code(link):\n",
        "    if \"github\" in link:\n",
        "        return \"GitHub\"\n",
        "    elif link == NONE_FIRST_VIEW:\n",
        "        return \"None\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "for sheet in codedfs:\n",
        "    df = codedfs[sheet]\n",
        "    code_series = df[LINK_TO_CODE].apply(classify_code).value_counts()\n",
        "\n",
        "\n",
        "    dset = \"ACL\" if sheet == GENERATION else \"INLG\"\n",
        "    for k, v in code_series.items():\n",
        "        if k == \"None\":\n",
        "            counts.append((label, dset, \"Not Published\", v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COQS3fIuCwqF"
      },
      "outputs": [],
      "source": [
        "label = \"Installation Instructions\"\n",
        "for sheet in codedfs:\n",
        "    df = codedfs[sheet]\n",
        "    df['Installation'] = df['Installation'].fillna('None')\n",
        "    print(f\"Unique values for 'Installation' in {sheet}: {df['Installation'].unique()}\")\n",
        "    installation_series = df['Installation'].value_counts()\n",
        "\n",
        "    dset = \"ACL\" if sheet == GENERATION else \"INLG\"\n",
        "    for k, v in installation_series.items():\n",
        "        counts.append((label, dset, k, v))\n",
        "\n",
        "    installation_series.plot(kind='pie', autopct='%1.1f%%')\n",
        "    plt.title(f\"'Installation' distribution in {sheet}\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"fig:install_instr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maLTJ46yDiCu"
      },
      "outputs": [],
      "source": [
        "label = f\"Found Experiments\"\n",
        "for sheet in codedfs:\n",
        "    df = codedfs[sheet]\n",
        "    df['Experiments Covered'] = df['Experiments Covered'].fillna('None')\n",
        "    print(f\"Unique values for 'Experiments Covered' in {sheet}: {df['Experiments Covered'].unique()}\")\n",
        "    experiments_series = df['Experiments Covered'].value_counts()\n",
        "    experiments_series.plot(kind='pie', autopct='%1.1f%%')\n",
        "\n",
        "    dset = \"ACL\" if sheet == GENERATION else \"INLG\"\n",
        "    for k, v in experiments_series.items():\n",
        "        counts.append((label, dset, k, v))\n",
        "\n",
        "\n",
        "    plt.title(f\"'Experiments Covered' distribution in {sheet}\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"fig_experiments\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko3LuUJhDqVB"
      },
      "outputs": [],
      "source": [
        "label = f\"Documentation of Code\"\n",
        "\n",
        "for sheet in codedfs:\n",
        "    df = codedfs[sheet]\n",
        "    df['Scripts Documentation'] = df['Scripts Documentation'].fillna('None')\n",
        "    print(f\"Unique values for 'Scripts Documentation' in {sheet}: {df['Scripts Documentation'].unique()}\")\n",
        "    scripts_doc_series = df['Scripts Documentation'].value_counts().sort_index()\n",
        "    scripts_doc_series.plot(kind='pie', autopct='%1.1f%%')\n",
        "\n",
        "    dset = \"ACL\" if sheet == GENERATION else \"INLG\"\n",
        "    for k, v in scripts_doc_series.items():\n",
        "        counts.append((label, dset, k, v))\n",
        "    plt.title(f\"'Scripts Documentation' distribution in {sheet}\")\n",
        "    plt.show()\n",
        "    print(\"fig:documentation\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confs = [t[1] for t in counts]\n",
        "\n",
        "# fig, axs = plt.subplots(len(labels), figsize=(10, 15))\n",
        "\n",
        "label = 'Open Source?'\n",
        "d = {}\n",
        "for conf in confs:\n",
        "  d[f\"{conf} 2023\"] = dict(t[2:4] for t in counts if t[1] == conf and t[0] == label)\n",
        "comparison = pd.DataFrame(d).fillna(0).T\n",
        "\n",
        "ax = comparison.plot(kind='bar', stacked=True, color=[\"#e5c185\", \"#b8cdab\", \"#779af5\"], figsize=(5,5))\n",
        "\n",
        "labels = list(d[f\"{confs[0]} 2023\"].keys())\n",
        "\n",
        "plt.ylabel('# of Papers')\n",
        "\n",
        "# Shrink current axis's height by 10% on the bottom\n",
        "box = ax.get_position()\n",
        "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
        "              box.width, box.height * 0.9])\n",
        "\n",
        "plt.legend(title=label, loc='upper left', labels=labels, ncols=2)\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.legend(title='Was code delivered?', loc='upper left', ncols=1)\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.tight_layout()\n",
        "\n",
        "totals = comparison.sum(axis=1)\n",
        "\n",
        "# Annotate the bars with the percentage values\n",
        "for p in ax.patches:\n",
        "    width, height = p.get_width(), p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "\n",
        "    # Get the conference corresponding to this bar\n",
        "    conference = ax.get_xticklabels()[int(x + width / 2)].get_text()\n",
        "\n",
        "    # Calculate the percentage relative to the total for that conference\n",
        "    total = totals[conference]\n",
        "    percentage = (height / total) * 100 if total > 0 else 0\n",
        "\n",
        "    # Display the percentage\n",
        "    if height > 0:\n",
        "        ax.text(x + width / 2, y + - 0.1 + height / 2, f'{percentage:.1f}%', ha='center', va='center', fontsize=10)\n",
        "\n",
        "\n",
        "plt.show()\n",
        "print(\"fig_nocode_deliver_promised\")"
      ],
      "metadata": {
        "id": "Q7FLokygC2i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confs = [t[1] for t in counts]\n",
        "label = 'Documentation of Code'\n",
        "d = {}\n",
        "\n",
        "for conf in confs:\n",
        "  d[f\"{conf} 2023\"] = dict(t[2:4] for t in counts if t[1] == conf and t[0] == label)\n",
        "comparison = pd.DataFrame(d).fillna(0).T\n",
        "\n",
        "\n",
        "ax = comparison.plot(kind='bar', stacked=True, color=[\"#e5c185\", \"#b8cdab\", \"#779af5\"], figsize=(5,5))\n",
        "\n",
        "labels = list(d[f\"{confs[0]} 2023\"].keys())\n",
        "\n",
        "plt.ylabel('# of Papers')\n",
        "\n",
        "# Shrink current axis's height by 10% on the bottom\n",
        "box = ax.get_position()\n",
        "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
        "              box.width, box.height * 0.9])\n",
        "\n",
        "plt.legend(title=\"Documentation Status\", loc='upper left', labels=labels, ncols=1)\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.tight_layout()\n",
        "\n",
        "totals = comparison.sum(axis=1)\n",
        "\n",
        "# Annotate the bars with the percentage values\n",
        "for p in ax.patches:\n",
        "    width, height = p.get_width(), p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "\n",
        "    # Get the conference corresponding to this bar\n",
        "    conference = ax.get_xticklabels()[int(x + width / 2)].get_text()\n",
        "\n",
        "    # Calculate the percentage relative to the total for that conference\n",
        "    total = totals[conference]\n",
        "    percentage = (height / total) * 100 if total > 0 else 0\n",
        "\n",
        "    # Display the percentage\n",
        "    if height > 0:\n",
        "        ax.text(x + width / 2, y + - 0.1 + height / 2, f'{percentage:.1f}%', ha='center', va='center', fontsize=10)\n",
        "\n",
        "\n",
        "plt.show()\n",
        "print(\"fig_documentation\")"
      ],
      "metadata": {
        "id": "tJibH2CBGqFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confs = [t[1] for t in counts]\n",
        "label = 'Installation Instructions'\n",
        "d = {}\n",
        "\n",
        "for conf in confs:\n",
        "  d[f\"{conf} 2023\"] = dict(t[2:4] for t in counts if t[1] == conf and t[0] == label)\n",
        "comparison = pd.DataFrame(d).fillna(0).T\n",
        "\n",
        "ax = comparison.plot(kind='bar', stacked=True, color=[\"#e5c185\", \"#b8cdab\", \"#779af5\"], figsize=(5,5))\n",
        "\n",
        "labels = list(d[f\"{confs[0]} 2023\"].keys())\n",
        "\n",
        "plt.ylabel('# of Papers')\n",
        "\n",
        "# Shrink current axis's height by 10% on the bottom\n",
        "box = ax.get_position()\n",
        "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
        "              box.width, box.height * 0.9])\n",
        "\n",
        "plt.legend(title=\"Installation Instructions\", loc='upper left', labels=labels, ncols=1)\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.tight_layout()\n",
        "\n",
        "totals = comparison.sum(axis=1)\n",
        "\n",
        "# Annotate the bars with the percentage values\n",
        "for p in ax.patches:\n",
        "    width, height = p.get_width(), p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "\n",
        "    # Get the conference corresponding to this bar\n",
        "    conference = ax.get_xticklabels()[int(x + width / 2)].get_text()\n",
        "\n",
        "    # Calculate the percentage relative to the total for that conference\n",
        "    total = totals[conference]\n",
        "    percentage = (height / total) * 100 if total > 0 else 0\n",
        "\n",
        "    # Display the percentage\n",
        "    if height > 0:\n",
        "        ax.text(x + width / 2, y + - 0.1 + height / 2, f'{percentage:.1f}%', ha='center', va='center', fontsize=10)\n",
        "\n",
        "\n",
        "plt.show()\n",
        "print(\"fig_install_instr\")"
      ],
      "metadata": {
        "id": "Q21doYv6HzFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confs = [t[1] for t in counts]\n",
        "label = 'Found Experiments'\n",
        "d = {}\n",
        "\n",
        "for conf in confs:\n",
        "  d[f\"{conf} 2023\"] = dict(t[2:4] for t in counts if t[1] == conf and t[0] == label)\n",
        "comparison = pd.DataFrame(d).fillna(0).T\n",
        "\n",
        "\n",
        "#comparison_normalized = comparison.div(comparison.sum(axis=1), axis=0) * 100\n",
        "ax = comparison.plot(kind='bar', stacked=True, color=[\"#e5c185\", \"#b8cdab\", \"#779af5\"], figsize=(5,5))\n",
        "\n",
        "labels = list(d[f\"{confs[0]} 2023\"].keys())\n",
        "\n",
        "plt.ylabel('# of Papers')\n",
        "\n",
        "# Shrink current axis's height by 10% on the bottom\n",
        "box = ax.get_position()\n",
        "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
        "              box.width, box.height * 0.9])\n",
        "\n",
        "plt.legend(title=\"Code for Experiments\", loc='upper left', labels=labels, ncols=1)\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.tight_layout()\n",
        "\n",
        "totals = comparison.sum(axis=1)\n",
        "\n",
        "# Annotate the bars with the percentage values\n",
        "for p in ax.patches:\n",
        "    width, height = p.get_width(), p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "\n",
        "    # Get the conference corresponding to this bar\n",
        "    conference = ax.get_xticklabels()[int(x + width / 2)].get_text()\n",
        "\n",
        "    # Calculate the percentage relative to the total for that conference\n",
        "    total = totals[conference]\n",
        "    percentage = (height / total) * 100 if total > 0 else 0\n",
        "\n",
        "    # Display the percentage\n",
        "    if height > 0:\n",
        "        ax.text(x + width / 2, y + - 0.1 + height / 2, f'{percentage:.1f}%', ha='center', va='center', fontsize=10)\n",
        "\n",
        "\n",
        "plt.show()\n",
        "print(\"fig_experiments\")"
      ],
      "metadata": {
        "id": "LxGIVcXAIJY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2tQssy2eOlc"
      },
      "source": [
        "## Main analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg7UFZuJuOCl"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUpR5Yjx6azo"
      },
      "outputs": [],
      "source": [
        "# Sheet names to analyze\n",
        "SHEETS = [\"INLG 2023\", \"ACL 2023\"]\n",
        "PROPERTIES = \"Metric properties\"\n",
        "\n",
        "# List the column names as constants to prevent typos\n",
        "ID = \"ACL Paper ID\"\n",
        "METRIC_NAME = \"Metric name\"\n",
        "NEWLY = \"Newly introduced?\"\n",
        "APPENDIX = \"Appendix\"\n",
        "TASK = \"Updated Task\"\n",
        "#TASK_OLD = \"Task\"\n",
        "#INK_TO_METRIC = \"Link to the Metric Paper\"\n",
        "#PAPER_LINK = \"Link to the Paper\"\n",
        "CORRELATED = \"Corrleated w/ Human Evaluation?\"\n",
        "ANNOTATOR = \"Annotator\"\n",
        "METRIC_IMPL = \"Metric Implementations\"\n",
        "IMPL = \"Metric Implementations\"\n",
        "RATIONALE = \"Notes: Rational\"\n",
        "COMMENTS = \"Comments\"\n",
        "CONF = \"Conf\"\n",
        "SURVEY = \"Survey\"\n",
        "FAMILY = \"Metric Family\"\n",
        "DISPLAY = 'Display Name'\n",
        "NORM = 'Normalized name'\n",
        "TRAIN = 'Trainable?'\n",
        "SRC = 'Uses source?'\n",
        "REF = 'Uses ref?'\n",
        "SRC_REF = 'Uses source or reference?'\n",
        "ADHOC = 'Is ad-hoc?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXvAiUGb_IlZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.colors as mcolors\n",
        "import re\n",
        "# The following snippet checks for the number of annotations per one paper per annotator and reports discrepancies\n",
        "def print_metric_counts(df):\n",
        "  counts = df.groupby([ID, ANNOTATOR]).size().reset_index(name='count')\n",
        "  agreed = disagreed = 0\n",
        "  for i, group in counts.groupby(ID):\n",
        "      unique_counts = group['count'].nunique()\n",
        "      if unique_counts > 1:\n",
        "          disagreed += 1\n",
        "          annotations_info = []\n",
        "          for annotator, count in zip(group[ANNOTATOR], group['count']):\n",
        "              annotations_info.append(f\"{annotator} reported {count} metrics\")\n",
        "          print(f\"For paper id {i}, {'; '.join(annotations_info)}\")\n",
        "      else:\n",
        "        agreed += 1\n",
        "  print(f'With {agreed} agreements and {disagreed} disagreements, annotators agreed in {100 * agreed / float(agreed + disagreed)}% cases.')\n",
        "\n",
        "# Normalize the metric string\n",
        "def normalize_metric(metric):\n",
        "  metric = re.sub('[- +@]+', '', metric) # Remove spaces and other special symbols that might occur - keeping parentheses deliberately\n",
        "  metric = metric.lower() # Lowercase everything\n",
        "  metric = re.sub('(#survey|\\(corpus\\))', '', metric)\n",
        "  if metric in names_to_split.keys():\n",
        "    return names_to_split[metric]\n",
        "  if metric in metric_mapping.keys():\n",
        "    return metric_mapping[metric]\n",
        "  return metric\n",
        "\n",
        "# Some metrics were reported as several metrics in one line, split them to keep them consistent\n",
        "def split_grouped_metrics(df):\n",
        "  return df.explode(METRIC_NAME)\n",
        "\n",
        "def assign_family(metric):\n",
        "  if metric in metric_families.keys():\n",
        "    return metric_families[metric]\n",
        "  if 'human' in metric:\n",
        "    return 'Human'\n",
        "  return metric\n",
        "\n",
        "# Normalize URLs\n",
        "def normalize_urls(url):\n",
        "  return re.sub('(/|\\.pdf)$', '', url)\n",
        "\n",
        "def normalize_task(taskstring):\n",
        "  if taskstring is None:\n",
        "    return frozenset()\n",
        "  if isinstance(taskstring, frozenset):\n",
        "    return taskstring\n",
        "  subtasks = re.split('[:;,]', taskstring)\n",
        "  updated = []\n",
        "  for t in subtasks:\n",
        "    task = t.strip()\n",
        "    if task != \"\":\n",
        "      if task in task_mapping.keys():\n",
        "        task = task_mapping[task]\n",
        "      updated.append(task)\n",
        "\n",
        "  return frozenset(updated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byYdKb8Xsg7r"
      },
      "source": [
        "### Data loading\n",
        "Open the worksheet and make it into a DataFrame + Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UIiWDyGoCDr"
      },
      "outputs": [],
      "source": [
        "def sheet2df(sheet):\n",
        "  worksheet = gc.open_by_url('https://docs.google.com/spreadsheets/d/10Yvxn7sb78cZmpmM0RYoAuVHoaP_KiDc970vg9pSa2c/').worksheet(sheet)\n",
        "\n",
        "  # get_all_values gives a list of rows.\n",
        "  rows = worksheet.get_all_values()\n",
        "\n",
        "  # Convert to a DataFrame and render.\n",
        "  df = pd.DataFrame.from_records(rows[1:], columns=rows[0])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpxEFlrjs3Ih"
      },
      "outputs": [],
      "source": [
        "# Loading all the annotation data\n",
        "dfs = []\n",
        "for sheet in SHEETS:\n",
        "  df = sheet2df(sheet)\n",
        "  # Sometimes there will be blank rows with \"Updated Task\"\n",
        "  df = df[df[ID] != \"\"]\n",
        "  df[CONF] = sheet\n",
        "  df.reset_index(inplace=True, drop=True)\n",
        "  dfs.append(df)\n",
        "df = pd.concat(dfs, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zna0K_vunvSS"
      },
      "outputs": [],
      "source": [
        "# Making paper URLs easy to get -- getting them from the other sheets\n",
        "papers_list = pd.concat([sheet2df('Generation')[['ACL ID', 'Paper Link']], sheet2df('INLG 2023 Papers')[['ACL ID', 'Paper Link']]], ignore_index=True)\n",
        "\n",
        "def id2link(paper_id):\n",
        "  try:\n",
        "    return list(papers_list[papers_list['ACL ID'] == paper_id]['Paper Link'])[0]\n",
        "  except:\n",
        "    return paper_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6WUxwUvvSik"
      },
      "source": [
        "### Checksums & normalizations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metric name normalization"
      ],
      "metadata": {
        "id": "zxgBqF6G9Kay"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvjKK0Es-OPH"
      },
      "outputs": [],
      "source": [
        "names_to_split = {\n",
        "    \"accuracy/p/r/f1\": [\"accuracy\", \"precision\", \"recall\", \"f1\"],\n",
        "    \"bleu{1,2}\": [\"bleu1\", \"bleu2\"],\n",
        "    \"distinctngrams(dist{1,2,3})\": ['distinctunigrams', 'distinctbigrams', 'distincttrigrams'],\n",
        "    \"dist{1,2,3}\": ['distinctunigrams', 'distinctbigrams', 'distincttrigrams'],\n",
        "    \"repnmetricsforn=2,3,4\": ['bigramrepetition', 'trigramrepetition', '4gramrepetition'],\n",
        "    \"rouge{1,2,l}\": [\"rouge1\", \"rouge2\", \"rougel\"],\n",
        "    \"rouge{1,2}\": [\"rouge1\", \"rouge2\"],\n",
        "    \"human(fluency,faithfulness,coverage,repetition)\": [\"human(fluency)\", \"human(faithfulness)\", \"human(coverage)\", \"human(repetition)\"],\n",
        "    \"human(fluency,relatedness,correctness,diversity)\": [\"human(fluency)\", \"human(relatedness)\", \"human(correctness)\", \"human(diversity)\"],\n",
        "    \"human(rationality,fluency)\": [\"human(rationality)\", \"human(fluency)\"],\n",
        "    \"human(simplicitiy,correctness,fluency)\": [\"human(simplicity)\", \"human(correctness)\", \"human(fluency)\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTcwB4QF3nyU"
      },
      "outputs": [],
      "source": [
        "metric_mapping = {\n",
        "    'harmonicmean(hmean)between(1−pbleu)andbleu': 'harmonicmean(pbleubleu)',\n",
        "    'harmonicmeanof1pbleuandbleu': 'harmonicmean(pbleubleu)',\n",
        "    'hmeanbetween(1pbleu)andbleu':'harmonicmean(pbleubleu)',\n",
        "    'harmonicmeanofbleu4andstyleaccuracy': 'harmonicmean(bleu4styleaccuracy)',\n",
        "    'pairwisebleu': 'pbleu',\n",
        "    'pbleu(selfbleu)': 'pbleu',\n",
        "    'em': 'exactmatch',\n",
        "    'exactmatch(em)': 'exactmatch',\n",
        "    'inform(rate)': 'inform',\n",
        "    'success(rate)': 'success',\n",
        "    'combinescore(informandrate)': 'combinedscore(informandrate)',\n",
        "    'bleu(4)': 'bleu4',\n",
        "    'accuracy(?)': 'accuracy',\n",
        "    'macroaveragedf1score(f1)': 'f1',\n",
        "    'sensitivity': 'demetrbenchmarksensitivityscores',\n",
        "    'bleurtbase': 'bleurt',\n",
        "    'allmpnetbasev2': 'mpnetcosinesimilarity',\n",
        "    'negmpnet': 'negmpnetcosinesimilarity',\n",
        "    'distinct1': 'distinctunigrams',\n",
        "    'distinct2': 'distinctbigrams',\n",
        "    'distinct4': 'distinct4grams',\n",
        "    'dist1': 'distinctunigrams',\n",
        "    'dist2': 'distinctbigrams',\n",
        "    'dist3': 'distincttrigrams',\n",
        "    'distinct3': 'distincttrigrams',\n",
        "    \"distn(4?)\": 'distinct4grams',\n",
        "    'bleuscore': 'bleu',\n",
        "    'corpusbleu': 'bleu',\n",
        "    'rquge': 'rouge',\n",
        "    'bertscorefscore': 'bertscoref1',\n",
        "    'bertscorep': 'bertscoreprecision',\n",
        "    'bertscorer': 'bertscorerecall',\n",
        "    'beatf1': 'bertscoref1',\n",
        "    'bertscorefmeasure': 'bertscoref1',\n",
        "    'bleurtscore': 'bleurt',\n",
        "    'human(creativeness)': 'human(creativity)',\n",
        "    'human(informativity)': 'human(informativeness)',\n",
        "    'grammar(gram)': 'grammaticality',\n",
        "    'human(intrestingness)': 'human(interesting)',\n",
        "    'human(overalquality/preference)': 'human(overall)',\n",
        "    'humanfluency': 'human(fluency)',\n",
        "    'humaninformativeness': 'human(informativeness)',\n",
        "    'lr(lexicalrepetition)': 'lexicalrepetition',\n",
        "    'mauvescore': 'mauve',\n",
        "    'rouge2(r2)': 'rouge',\n",
        "    'cosinedistance': 'cosinesimilarity',\n",
        "    'auroc': 'auc',\n",
        "    'detoxification': 'detoxify',\n",
        "    'fleschkincaidgradelevelreadability(fkgl)': 'fleschkincaidgradelevel',\n",
        "    'fkgl': 'fleschkincaidgradelevel',\n",
        "    'fleschkincaidgradelevel(fkgl)': 'fleschkincaidgradelevel',\n",
        "    'repeatedtrigrams': 'trigramrepetition'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnod6kamyAtD"
      },
      "outputs": [],
      "source": [
        "task_mapping = {\n",
        "    \"natural language entailment)\": \"natural language inference\",\n",
        "    \"data-text generation\": \"data-to-text generation\",\n",
        "    \"data-to-text\": \"data-to-text generation\",\n",
        "    \"dialogue generation\": \"dialogue turn generation\",\n",
        "    \"dialogue response\": \"dialogue turn generation\",\n",
        "    \"dialouge\": \"dialogue turn generation\",\n",
        "    \"open-ended dialogue\": \"dialogue turn generation\",\n",
        "    \"task-oriented dialouge\": \"dialogue turn generation\",\n",
        "    \"paraphrase generation\": \"paraphrasing / lossless simplification\",\n",
        "    \"paraphrasing/lossless simplification\": \"paraphrasing / lossless simplification\",\n",
        "    \"text simplification\": \"compression / lossy simplification\",\n",
        "    \"question-generation\": \"question generation\",\n",
        "    \"quora question pairs\": \"question answering\",\n",
        "    \"and question answering\": \"question answering\",\n",
        "    \"simile generation\": \"simile generation (text-to-text)\",\n",
        "    \"story-generation\": \"story generation\",\n",
        "    \"text summarization\": \"summarisation (text-to-text)\",\n",
        "    \"summarisation\": \"summarisation (text-to-text)\",\n",
        "    \"summarization\": \"summarisation (text-to-text)\",\n",
        "    \"summarization (text-to-text)\": \"summarisation (text-to-text)\",\n",
        "    \"evaluate semantic diversity between two natural language \\ngeneration\": \"evaluate semantic diversity between two natural language generation\",\n",
        "    \"Updated Task\": \"\",\n",
        "    \"translation\": \"machine translation\",\n",
        "    \"surface realisation (slr to text)\": \"surface realisation (SLR to text)\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9O0NAdnfONS"
      },
      "outputs": [],
      "source": [
        "# Get Metric Information from the Properties sheet\n",
        "def add_properties(df):\n",
        "  properties = sheet2df(PROPERTIES)\n",
        "  properties[ID] = properties['Paper IDs'].apply(lambda x: x.split())\n",
        "  properties = properties.explode(ID)\n",
        "  df = df.merge(properties, left_on=[METRIC_NAME, ID], right_on=[METRIC_NAME, ID], how='left')\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksciZvly8_px"
      },
      "outputs": [],
      "source": [
        "# Normalize\n",
        "df[SURVEY] = df[METRIC_NAME].str.contains(\"#survey\")\n",
        "df[METRIC_NAME] = df[METRIC_NAME].apply(normalize_metric)\n",
        "df[TASK] = df[TASK].apply(normalize_task)\n",
        "df = split_grouped_metrics(df)\n",
        "\n",
        "df = add_properties(df)\n",
        "df[FAMILY] = df[FAMILY].fillna(\"Human\")\n",
        "df = df.fillna('')\n",
        "\n",
        "# Leaving the surveys out of the analysis\n",
        "df = df[df[SURVEY] == False]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAJmOhYgtCjE"
      },
      "source": [
        "#### Basic paper counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al0Hi9ur6j8G"
      },
      "outputs": [],
      "source": [
        "# Get rid of some small categories\n",
        "print(f\"There are {len(df[df[CORRELATED] == 'Correlation with previous human eval'])} metrics with a correlation to previous human eval\")\n",
        "df = df.replace(\"Correlation with previous human eval\", \"Human evaluation, quantitative correlation\")\n",
        "\n",
        "\n",
        "# First take a look at papers with no metrics at all\n",
        "no_metrics = df[df[METRIC_NAME] == \"\"]\n",
        "num_no_metrics = len(no_metrics[ID].unique())\n",
        "df_all = df[df[METRIC_NAME] != \"\"]\n",
        "num_with_all = len(df_all[ID].unique())\n",
        "print(f\"There are {num_no_metrics} papers with no metrics, {num_with_all} papers remain for analysis.\")\n",
        "\n",
        "\n",
        "# Now exclude papers with only human metrics, but also report how many papers uses human metrics\n",
        "hum_df = df[df[METRIC_NAME].str.contains('human')]\n",
        "papers_hum = len(hum_df[ID].unique())\n",
        "num_h = len(hum_df)\n",
        "dist_h = len(hum_df[METRIC_NAME].unique())\n",
        "print(f\"{papers_hum} out of {len(df[ID].unique())} papers use human evaluation. In total, there were {num_h} instances of human metrics used, {dist_h} of those are unique.\")\n",
        "\n",
        "auto_df = df_all[~df_all[METRIC_NAME].str.contains('human')]\n",
        "papers_auto = len(auto_df[ID].unique())\n",
        "num_a = len(auto_df)\n",
        "dist_a = len(auto_df[METRIC_NAME].unique())\n",
        "print(f\"{papers_auto} out of {len(df[ID].unique())} papers use automatic evaluation. In total, there were {num_a} instances of automatic metrics used, {dist_a} of those are unique.\")\n",
        "\n",
        "num_fam = len(auto_df[FAMILY].unique())\n",
        "print(f\"There are {num_fam} metric families.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y08T6SoEaInL"
      },
      "outputs": [],
      "source": [
        "# Which papers were eliminated?\n",
        "paper_ids_with_metrics = df_all[ID].unique().tolist()\n",
        "missing_papers_df = papers_list[~papers_list[\"ACL ID\"].isin(paper_ids_with_metrics)]\n",
        "missing_papers_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GISeT7jsWodS"
      },
      "outputs": [],
      "source": [
        "# How many papers contain both human and automatic metrics:\n",
        "hum_paper_ids = hum_df[ID].unique().tolist()\n",
        "auto_paper_ids = auto_df[ID].unique().tolist()\n",
        "print(f\"Number of papers that have both automatic and human evaluations: {len(set(auto_paper_ids).intersection(set(hum_paper_ids)))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVHJpWsCs42B"
      },
      "source": [
        "#### Metric properties sheet creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JATEklD5hw3o"
      },
      "outputs": [],
      "source": [
        "# For debugging the metric families\n",
        "by_family = {}\n",
        "for entry in df[[ID, NEWLY, METRIC_NAME,FAMILY]].to_dict('records'):\n",
        "  by_family[entry[FAMILY]] = by_family.get(entry[FAMILY], [])\n",
        "  by_family[entry[FAMILY]].append({METRIC_NAME: entry[METRIC_NAME], ID: entry[ID], 'URL': id2link(entry[ID]), NEWLY: entry[NEWLY]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQEgyZ6IM8VJ"
      },
      "outputs": [],
      "source": [
        "# This is the code that produced the base of the \"Metric properties\" sheet\n",
        "import csv\n",
        "\n",
        "data_rows = []\n",
        "\n",
        "for key, values in by_family.items():\n",
        "  metric_names = {}\n",
        "  # We only care about automatic metrics in this part\n",
        "  if key == \"Human\":\n",
        "    continue\n",
        "  for value in values:\n",
        "    # Multiple occurrences of the same metric: just add IDs & URLs to the 1st mention\n",
        "    if value[METRIC_NAME] in metric_names:\n",
        "      data_rows[metric_names[value[METRIC_NAME]]][-3] += 1\n",
        "      data_rows[metric_names[value[METRIC_NAME]]][-2] += ' ' + value[ID]\n",
        "      data_rows[metric_names[value[METRIC_NAME]]][-1] += ' ' + value['URL']\n",
        "      continue\n",
        "    # 1st occurrence: create a new row\n",
        "    row = [value[METRIC_NAME], key, None, None, None, None, 1, value[ID], value['URL']]\n",
        "    metric_names[value[METRIC_NAME]] = len(data_rows)  # store ref to row where it's introduced\n",
        "    data_rows.append(row)\n",
        "\n",
        "with open('dict.csv', 'w') as csv_file:\n",
        "    writer = csv.writer(csv_file, delimiter='\\t')\n",
        "    for row in data_rows:\n",
        "      writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analyzing tasks\n",
        "\n",
        "Compared to the originally defined tasks, we needed to introduce more. This code was used to analyze and unify them."
      ],
      "metadata": {
        "id": "rD6TvY2n8pr9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtSDqyKo4EGn"
      },
      "outputs": [],
      "source": [
        "uni = set ()\n",
        "for fs in df[TASK].unique():\n",
        "  uni.update(fs)\n",
        "uni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGar75G8fncw"
      },
      "outputs": [],
      "source": [
        "defined_tasks = {'data-to-text generation',\n",
        "'dialogue turn generation',\n",
        "'content selection/determination',\n",
        "'content ordering/structuring',\n",
        "'deep generation (DLR to text)',\n",
        "'aggregation',\n",
        "'lexicalisation',\n",
        "'referring expression generation',\n",
        "'surface realisation (SLR to text)',\n",
        "'feature-controlled generation',\n",
        "'question generation',\n",
        "'question answering',\n",
        "'paraphrasing / lossless simplification',\n",
        "'machine translation',\n",
        "'summarisation (text-to-text)',\n",
        "'compression / lossy simplification',\n",
        "'end-to-end text generation',\n",
        "'multiple (list all)',\n",
        "'other (please specify)'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nmm1nIb-f6Bw"
      },
      "outputs": [],
      "source": [
        "other = uni.difference(defined_tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxMn4yVgg7En"
      },
      "outputs": [],
      "source": [
        "task_df = df.copy(deep=True)\n",
        "task_df[TASK] = task_df[TASK].apply(lambda x: list(x))\n",
        "task_df = task_df.explode(TASK).drop_duplicates(subset=[ID, TASK])\n",
        "other_tasks = task_df[task_df[TASK].isin(other)]\n",
        "other_tasks[TASK].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e25GN4idiyoJ"
      },
      "outputs": [],
      "source": [
        "other_tasks[other_tasks[TASK] == \"open-ended text generation (LM sampling)\"][['Annotator_x', ID, TASK, 'Link to the Paper']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axeuFLnbkY9-"
      },
      "outputs": [],
      "source": [
        "task_df[TASK].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM_GVoTYveVi"
      },
      "source": [
        "### Stats computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8xIQjF9xxLQ"
      },
      "source": [
        "#### Basic count Venn diagrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4DLxqDdGQgK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib_venn import venn2\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5), facecolor='none')\n",
        "\n",
        "\n",
        "# ACL 2023 Venn diagram\n",
        "axs[0].set_title(\"ACL 2023\")\n",
        "venn_acl = venn2(\n",
        "    subsets=(\n",
        "        set(hum_df[hum_df[CONF] == \"ACL 2023\"][ID].unique()),\n",
        "        set(auto_df[auto_df[CONF] == \"ACL 2023\"][ID].unique())\n",
        "    ),\n",
        "    set_labels=[\"Human Evaluation\", \"Automatic Evaluation\"],\n",
        "    set_colors=(\"#A682FF\", \"#55C1FF\"),\n",
        "    ax=axs[0]\n",
        ")\n",
        "\n",
        "#Setting the dashed line for perimeter\n",
        "for circle in venn_acl.patches:\n",
        "    if circle:\n",
        "        circle.set_edgecolor('black')\n",
        "        circle.set_linestyle('--')\n",
        "\n",
        "#manually aligning labels and titles\n",
        "venn_acl.set_labels[0].set(x=-0.2, y=-0.6)\n",
        "venn_acl.set_labels[1].set(x=0.2, y=-0.6)\n",
        "axs[0].set_title(\"ACL 2023\", x=0.5, y=1)\n",
        "\n",
        "# INLG 2023 Venn diagram\n",
        "axs[1].set_title(\"INLG 2023\")\n",
        "venn_inlg =venn2(\n",
        "    subsets=(\n",
        "        set(hum_df[hum_df[CONF] == \"INLG 2023\"][ID].unique()),\n",
        "        set(auto_df[auto_df[CONF] == \"INLG 2023\"][ID].unique())\n",
        "    ),\n",
        "    set_labels=[\"Human Evaluation\", \"Automatic Evaluation\"],\n",
        "    set_colors=(\"#D00000\", \"#FFBA08\"),\n",
        "    ax=axs[1]\n",
        ")\n",
        "\n",
        "#Setting the dashed line for perimeter\n",
        "for circle in venn_inlg.patches:\n",
        "    if circle:\n",
        "        circle.set_edgecolor('black')\n",
        "        circle.set_linestyle('--')\n",
        "axs[1].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "axs[1].patch.set_alpha(0)\n",
        "\n",
        "#manually aligning labels and titles\n",
        "venn_inlg.set_labels[0].set(x=-0.2, y=-0.62)\n",
        "venn_inlg.set_labels[1].set(x=0.16, y=-0.62)\n",
        "axs[1].set_title(\"INLG 2023\", x=0.5, y=1.04)\n",
        "\n",
        "# Adjust layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwbkxopLx2_9"
      },
      "source": [
        "#### Metric use count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBBLTB0MF493"
      },
      "outputs": [],
      "source": [
        "auto_df[FAMILY].value_counts().plot(kind='barh', figsize=(8, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUj-Wwy8yFa6"
      },
      "source": [
        "#### Metric family per venue (high-level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqtdRYV1k3HC"
      },
      "outputs": [],
      "source": [
        "F = \"MF2\"\n",
        "\n",
        "# Showing metric families as a grouped bar chart, only top-10 metrics (ranked from INLG) shown\n",
        "def plot_bar_chart_overlay_sorted_with_values(df1, df2, ax, title='', color1='blue', color2='red'):\n",
        "    # Use seaborn style\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Combine unique \"Metric Family\" values from both DataFrames\n",
        "    unique_values = np.union1d(df1[F].unique(), df2[F].unique())\n",
        "\n",
        "    # Count occurrences of each value in 'Metric Family' column for both DataFrames\n",
        "    counts_df1 = df1[F].value_counts().reindex(unique_values, fill_value=0)\n",
        "    counts_df2 = df2[F].value_counts().reindex(unique_values, fill_value=0)\n",
        "\n",
        "    # Sort \"Metric Family\" values based on counts in df1 and get the top 10\n",
        "    top_10_values = counts_df1.sort_values(ascending=False).head(10).index\n",
        "\n",
        "    # Sum up all other values into 'Other' category\n",
        "    other_count_df1 = counts_df1[~counts_df1.index.isin(top_10_values)].sum()\n",
        "    other_count_df2 = counts_df2[~counts_df2.index.isin(top_10_values)].sum()\n",
        "\n",
        "    # Add 'Other' category to the top 10 values\n",
        "    top_10_values = top_10_values.append(pd.Index(['Other']))\n",
        "\n",
        "    # Reindex counts based on top 10 \"Metric Family\" values plus 'Other'\n",
        "    counts_df1_sorted = pd.concat([counts_df1.reindex(top_10_values[:-1]), pd.Series({'Other': other_count_df1})])\n",
        "    counts_df2_sorted = pd.concat([counts_df2.reindex(top_10_values[:-1]), pd.Series({'Other': other_count_df2})])\n",
        "\n",
        "    # Convert counts to percentages\n",
        "    values_df1 = (counts_df1_sorted / counts_df1.sum() * 100).values\n",
        "    values_df2 = (counts_df2_sorted / counts_df2.sum() * 100).values\n",
        "    labels = top_10_values\n",
        "\n",
        "    x = np.arange(len(labels))  # the label locations\n",
        "    width = 0.35  # the width of the bars\n",
        "\n",
        "    # Plot data for df1 and df2\n",
        "    rects1 = ax.bar(x - width/2, values_df1, width, label='INLG 2023', color=color1, alpha=1)\n",
        "    rects2 = ax.bar(x + width/2, values_df2, width, label='ACL 2023', color=color2, alpha=1)\n",
        "\n",
        "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "    ax.set_ylabel('Percentage (%)')\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels, rotation=90)\n",
        "    ax.legend(loc='upper center')\n",
        "\n",
        "    # Attach a text label above each bar in *rects*, displaying its height.\n",
        "    def autolabel(rects):\n",
        "        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate(f'{height:.1f}%',\n",
        "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                        xytext=(0, 3),  # 3 points vertical offset\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom',\n",
        "                        size=8)\n",
        "\n",
        "    autolabel(rects1)\n",
        "    autolabel(rects2)\n",
        "# Assuming df1 and df2 are your two DataFrames\n",
        "df1 = auto_df[auto_df[CONF] == \"INLG 2023\"].drop_duplicates(subset=[ID, FAMILY])\n",
        "df2 = auto_df[auto_df[CONF] == \"ACL 2023\"].drop_duplicates(subset=[ID, FAMILY])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "plot_bar_chart_overlay_sorted_with_values(df1, df2, ax, title='Metric families usage across venues', color1='#bbe6ff',color2='#ec9999')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a5DDDSik8PB"
      },
      "source": [
        "#### Metric per venue (top 10 + rest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAofKwt1gSkH"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Filter the dataframes based on the given conditions\n",
        "df1 = auto_df[auto_df[CONF] == 'INLG 2023'].drop_duplicates(subset=[ID, FAMILY])\n",
        "df2 = auto_df[auto_df[CONF] == 'ACL 2023'].drop_duplicates(subset=[ID, FAMILY])\n",
        "\n",
        "df1_dict = df1[[FAMILY, 'MF2']].value_counts().to_dict()\n",
        "df2_dict = df2[[FAMILY, 'MF2']].value_counts().to_dict()\n",
        "\n",
        "# make the sorting normalized\n",
        "summed = dict(Counter((df1[[FAMILY, 'MF2']].value_counts() / len(df1)).to_dict()) + Counter((df2[[FAMILY, 'MF2']].value_counts() / len(df2)).to_dict()))\n",
        "\n",
        "hlf_counts = {hlf: sum({v for k, v in summed.items() if k[1] == hlf}) for hlf in {k[1] for k in summed}}\n",
        "# high-level families sorted by total frequency\n",
        "hlf_list = sorted(hlf_counts.keys(), key=lambda k: hlf_counts[k], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR-vhw6WYJvo"
      },
      "outputs": [],
      "source": [
        "# hierarchical: hlf -> family\n",
        "def get_hier_data(df_dict):\n",
        "  hier = {}\n",
        "  for k in summed:\n",
        "    hier[k[1]] = hier.get(k[1], {})\n",
        "    hier[k[1]][k[0]] = df_dict.get(k, 0)\n",
        "  maxlen = max(len(v) for v in hier.values())\n",
        "\n",
        "  vals = [[] for _ in range(maxlen)]\n",
        "  labels = [[] for _ in range(maxlen)]\n",
        "  for hlf in hlf_list:\n",
        "      g = list(sorted(hier[hlf].items(), key=lambda i: i[1], reverse=True))\n",
        "      g += [('', 0)] * (maxlen - len(g))\n",
        "      for i, (k, v) in enumerate(g):\n",
        "          labels[i].append(f'{hlf}-{k}' if k else '')\n",
        "          vals[i].append(v)\n",
        "  return vals, labels\n",
        "\n",
        "df1_hier = get_hier_data(df1_dict)\n",
        "df2_hier = get_hier_data(df2_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTqy0mJSYKsh"
      },
      "outputs": [],
      "source": [
        "# top-k + others split by hlf\n",
        "limit = 10\n",
        "mf_colors = [\"#c7522a\", \"#e5c185\", \"#fbf2c4\", \"#b8cdab\", \"#74a892\", \"#008585\", \"#4c9eb3\", \"#779af5\", \"#a59cff\", \"#dbcdf0\"]\n",
        "topk_items = [k for k, _ in sorted(summed.items(), key=lambda i: i[1], reverse=True)[:limit]]\n",
        "\n",
        "def get_topk_data(df_dict, colorscheme):\n",
        "\n",
        "  vals = [[df_dict.get(k, 0) for k in topk_items]]\n",
        "  labels = [[k[0] for k in topk_items]]\n",
        "  colors = [[colorscheme[hlf_list.index(k[1])] for k in topk_items]]\n",
        "\n",
        "  hlf_counts = {}\n",
        "  for k, v in df_dict.items():\n",
        "    if k in topk_items:\n",
        "      continue\n",
        "    hlf_counts[k[1]] = hlf_counts.get(k[1], 0) + v\n",
        "\n",
        "  for k, v in sorted(hlf_counts.items(), key=lambda i: i[1], reverse=True):\n",
        "    vals[-1].append(v)\n",
        "    labels[-1].append(\"Other \" + k)\n",
        "    colors[-1].append(colorscheme[hlf_list.index(k)])\n",
        "    vals.append([0] * len(topk_items))\n",
        "    labels.append([''] * len(topk_items))\n",
        "    colors.append([\"#000000\"] * len(topk_items))\n",
        "\n",
        "  vals.pop()\n",
        "  labels.pop()\n",
        "  colors.pop()\n",
        "\n",
        "  return vals, labels, colors\n",
        "\n",
        "df1_topk = get_topk_data(df1_dict, mf_colors)\n",
        "df2_topk = get_topk_data(df2_dict, mf_colors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsyAptryEWSW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# to make the pattern more subtle\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 7))\n",
        "\n",
        "def create_barplot(hlf_list, data, labels, colors, dflen, hatch, pos):\n",
        "\n",
        "  bottoms = np.zeros(len(hlf_list))\n",
        "  for ds, ls, cs in zip(data, labels, colors):\n",
        "    ds = np.array(ds) / dflen * 100  # percentage\n",
        "    axes.bar(np.arange(len(hlf_list)) - 0.15 + 0.3 * pos,\n",
        "             ds,\n",
        "             color=cs,\n",
        "             # position=pos,\n",
        "             width=0.3,\n",
        "             hatch=hatch,\n",
        "             edgecolor='white',\n",
        "             label=ls,\n",
        "             bottom=bottoms,\n",
        "             alpha=0.8,\n",
        "             zorder=10)\n",
        "    bottoms += ds\n",
        "\n",
        "  for x, y, s in zip(np.arange(len(hlf_list)) - 0.2 + 0.4 * pos,\n",
        "                     bottoms + 0.1, bottoms):\n",
        "    axes.text(x, y, f'{s:.1f}', fontsize='small', horizontalalignment='center', zorder=12)\n",
        "\n",
        "\n",
        "bars_list = [i[0] for i in topk_items] + ['Other']\n",
        "\n",
        "create_barplot(bars_list, df1_topk[0], df1_topk[1], df1_topk[2], len(df1), None, 0)\n",
        "create_barplot(bars_list, df2_topk[0], df2_topk[1], df2_topk[2], len(df2), 'xxxxx', 1)\n",
        "\n",
        "# Setting various params\n",
        "axes.grid(axis='y', zorder=0)\n",
        "axes.set_xlabel(\"Metric families\")\n",
        "axes.set_ylabel('% Papers Using')\n",
        "axes.yaxis.set_label_coords(-0.01, -0.18)  # move y axis label down to save space\n",
        "axes.set_title('Metric family use per venue')\n",
        "plt.xticks(range(len(bars_list)), bars_list, rotation='vertical')  # x axis labels: metric families\n",
        "\n",
        "\n",
        "legend = [Patch(facecolor='#808080', hatch=None, label='INLG 2023'),\n",
        "          Patch(facecolor='#808080', hatch='xxxxx', edgecolor='white', label='ACL 2023')]\n",
        "legend += [Patch(facecolor=c, label=hlf) for hlf, c in zip(hlf_list, mf_colors)]\n",
        "plt.legend(handles=legend, loc='upper left', ncol=2)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-nGXMnrB4zm"
      },
      "source": [
        "#### Correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3HItFlCiFXM"
      },
      "outputs": [],
      "source": [
        "inlg_corr = auto_df[auto_df[CONF] == \"INLG 2023\"].drop_duplicates(subset=[ID, CORRELATED])\n",
        "inlg_corr_counts = inlg_corr[CORRELATED].value_counts()\n",
        "\n",
        "\n",
        "acl_corr = auto_df[auto_df[CONF] == \"ACL 2023\"].drop_duplicates(subset=[ID, CORRELATED])\n",
        "acl_corr_counts = acl_corr[CORRELATED].value_counts()\n",
        "\n",
        "comparison = pd.DataFrame({'INLG 2023': inlg_corr_counts, 'ACL 2023': acl_corr_counts}).fillna(0).T\n",
        "comparison_normalized = comparison.div(comparison.sum(axis=1), axis=0) * 100\n",
        "palette = sns.color_palette([\"#c7522a\",\"#e5c185\",\"#fbf2c4\",\"#b8cdab\",\"#74a892\",\"#008585\",\"#4c9eb3\",\"#779af5\",\"#a59cff\",\"#dbcdf0\"], n_colors=4)\n",
        "ax = comparison_normalized.plot(kind='bar', stacked=True, color=palette, ylim=(0,100))\n",
        "\n",
        "labels = [\n",
        "    'No Correlation',\n",
        "    'No Human Evaluation',\n",
        "    'Qualitative Correlation',\n",
        "    'Quantitative Correlation',\n",
        "          ]\n",
        "\n",
        "plt.ylabel('Percentage')\n",
        "\n",
        "# Shrink current axis's height by 10% on the bottom\n",
        "box = ax.get_position()\n",
        "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
        "                 box.width, box.height * 0.9])\n",
        "\n",
        "#plt.title('Relative Makeup of Correlation with Human Evaluation', loc='center')\n",
        "plt.legend(title='Correlated with Human Evaluation?', bbox_to_anchor=(0.5, -0.1), loc='upper center',\n",
        "           labels=labels,\n",
        "           ncols=2)\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Annotate percentages on the bars\n",
        "for p in ax.patches:\n",
        "    width, height = p.get_width(), p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "    if height > 5:\n",
        "      ax.text(x + width / 2, y + height / 2, f'{height:.1f}%', ha='center', va='center')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Top 20 metrics per conference"
      ],
      "metadata": {
        "id": "qfS0cP7TTqtL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU0Q5R0Clo4z"
      },
      "outputs": [],
      "source": [
        "top_metrics = auto_df[auto_df[CONF] == \"ACL 2023\"][METRIC_NAME].value_counts().nlargest(20)\n",
        "\n",
        "top_metrics.plot(kind='barh')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xiu-r_bcnJ_k"
      },
      "outputs": [],
      "source": [
        "top_metrics = auto_df[auto_df[CONF] == \"INLG 2023\"][METRIC_NAME].value_counts().nlargest(20)\n",
        "\n",
        "top_metrics.plot(kind='barh')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Appendix-only metrics"
      ],
      "metadata": {
        "id": "vdLYnU0vTuUY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG0drh5vrgq0"
      },
      "outputs": [],
      "source": [
        "app = df_all[df_all[APPENDIX] == 'Yes']\n",
        "all_inlg = len(df_all[df_all[CONF] == \"INLG 2023\"])\n",
        "all_acl = len(df_all[df_all[CONF] == \"ACL 2023\"])\n",
        "app_inlg = len(app[app[CONF] == \"INLG 2023\"])\n",
        "app_acl = len(app[app[CONF] == \"ACL 2023\"])\n",
        "print(f'{app_inlg} metrics ({app_inlg / all_inlg * 100:.2f} %) were reported in the Appendix at INLG 2023.')\n",
        "print(f'{app_acl} metrics ({app_acl / all_acl * 100:.2f} %) were reported in the Appendix at ACL 2023')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE3tEcqp5wMx"
      },
      "outputs": [],
      "source": [
        "app"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementation details provided?"
      ],
      "metadata": {
        "id": "9Tj-XLHEUbMw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xal_8LOqLRrR"
      },
      "outputs": [],
      "source": [
        "# We noted down that there were two papers (P110 - 8 metrics, P312 - 5 metrics) that used multiple implementations\n",
        "# That is not enough for a graph, so we will merge them to \"Implementation details provided\"\n",
        "\n",
        "auto_df[IMPL] = auto_df[IMPL].replace(\"Multiple implementations used\", \"Implementation details provided\")\n",
        "\n",
        "inlg_impl = auto_df[auto_df[CONF] == \"INLG 2023\"] #.drop_duplicates(subset=[ID, FAMILY])\n",
        "inlg_impl_counts = inlg_impl[IMPL].value_counts()\n",
        "\n",
        "\n",
        "acl_impl = auto_df[auto_df[CONF] == \"ACL 2023\"] #.drop_duplicates(subset=[ID, FAMILY])\n",
        "acl_impl_counts = acl_impl[IMPL].value_counts()\n",
        "\n",
        "\n",
        "\n",
        "comparison = pd.DataFrame({'INLG 2023': inlg_impl_counts, 'ACL 2023': acl_impl_counts}).fillna(0).T\n",
        "comparison_normalized = comparison.div(comparison.sum(axis=1), axis=0) * 100\n",
        "ax = comparison_normalized.plot(kind='bar', stacked=True, ylim=(0,100))\n",
        "\n",
        "labels = [\"No\", \"Yes\"]\n",
        "\n",
        "plt.ylabel('Percentage')\n",
        "\n",
        "\n",
        "#plt.title('Relative Makeup of Correlation with Human Evaluation', loc='center')\n",
        "plt.legend(title='Did the authors provide implementation details in the paper?', bbox_to_anchor=(0.5, -0.1), loc='upper center', ncols=2, labels=labels)\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Annotate percentages on the bars\n",
        "for p in ax.patches:\n",
        "    width, height = p.get_width(), p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "    if height > 5:\n",
        "      ax.text(x + width / 2, y + height / 2, f'{height:.1f}%', ha='center', va='center')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n746wAjuvjg"
      },
      "outputs": [],
      "source": [
        "all_impl = auto_df\n",
        "bleu_impl = auto_df[auto_df[FAMILY] == \"BLEU\"].drop_duplicates(subset=[ID, FAMILY, IMPL])\n",
        "rouge_impl = auto_df[auto_df[FAMILY] == \"ROUGE\"].drop_duplicates(subset=[ID, FAMILY, IMPL])\n",
        "all_impl_counts = all_impl[IMPL].value_counts()\n",
        "bleu_impl_counts = bleu_impl[IMPL].value_counts()\n",
        "rouge_impl_counts = rouge_impl[IMPL].value_counts()\n",
        "\n",
        "comparison = pd.DataFrame({'BLEU': bleu_impl_counts, 'ROUGE': rouge_impl_counts}).fillna(0).T\n",
        "comparison_normalized = comparison.div(comparison.sum(axis=1), axis=0) * 100\n",
        "ax = comparison_normalized.plot(kind='bar', stacked=True, ylim=(0,100))\n",
        "\n",
        "labels = []\n",
        "\n",
        "plt.ylabel('Percentage')\n",
        "\n",
        "\n",
        "#plt.title('Relative Makeup of Correlation with Human Evaluation', loc='center')\n",
        "plt.legend(title='Type of Metric Used', bbox_to_anchor=(0.5, -0.1), loc='upper center', ncols=2)\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Annotate percentages on the bars\n",
        "for p in ax.patches:\n",
        "    width, height = p.get_width(), p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "    if height > 5:\n",
        "      ax.text(x + width / 2, y + height / 2, f'{height:.1f}%', ha='center', va='center')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BLEU & ROUGE variants"
      ],
      "metadata": {
        "id": "i38UpdU9Ukk6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buhJWVIeCA3I"
      },
      "outputs": [],
      "source": [
        "# What variants of BLEU are used?\n",
        "bleu_types = auto_df[auto_df[FAMILY] == \"BLEU\"].groupby(['Display Name'])['Display Name'].count()\n",
        "bleu_types = bleu_types.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x1m8CahAsRZ"
      },
      "outputs": [],
      "source": [
        "# What variants of ROUGE are used?\n",
        "rouge_types = auto_df[auto_df[FAMILY] == \"ROUGE\"].groupby(['Display Name'])['Display Name'].count()\n",
        "rouge_types = rouge_types.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYePTszIJJ8E"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
        "fig.set_figwidth(10)\n",
        "fig.tight_layout()\n",
        "bleu_types.plot(kind=\"bar\", xlabel=\"\", ylabel=\"Count\", color=\"#779af5\", ax=axes[0])\n",
        "rouge_types.plot(kind=\"bar\", xlabel=\"\", color=\"#c7522a\", ax=axes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUdy73oAFr4V"
      },
      "outputs": [],
      "source": [
        "impl_ids = {}\n",
        "for impl in auto_df[IMPL].unique():\n",
        "  impl_ids[impl] = list(auto_df[auto_df[IMPL] == impl][ID].unique())\n",
        "import json\n",
        "with open('impl_ids.json', 'w') as f:\n",
        "  json.dump(impl_ids, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rationales"
      ],
      "metadata": {
        "id": "zCuL4DryUrEA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC_R3oAF1jBE"
      },
      "outputs": [],
      "source": [
        "rationales = auto_df['Notes: Rational']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPrCaCDJ6zwR"
      },
      "outputs": [],
      "source": [
        "follower_count = 0\n",
        "correlate = 0\n",
        "empty = 0\n",
        "other = []\n",
        "vals = []\n",
        "v_bin = []\n",
        "import re\n",
        "for r in rationales:\n",
        "  rl =  r.strip().lower()\n",
        "  if r == \"\" or re.match(\"(none given\\.?|not given)\", rl):\n",
        "    empty += 1\n",
        "    vals.append('None')\n",
        "    v_bin.append(0)\n",
        "  elif 'correlat' in rl:\n",
        "    correlate += 1\n",
        "    vals.append('Correlation')\n",
        "    v_bin.append(1)\n",
        "  elif re.search('(recent|previous|earlier|following|widely|staple|commonly|conventional |20\\d\\d)', rl):\n",
        "    follower_count += 1\n",
        "    vals.append('Following')\n",
        "    v_bin.append(1)\n",
        "  else:\n",
        "    other.append(r)\n",
        "    vals.append('Quality')\n",
        "    v_bin.append(1)\n",
        "print(f'{follower_count} ({follower_count/len(rationales)*100:.1f} %) metrics were used because the authors followed previous work.')\n",
        "print(f'{correlate} ({correlate/len(rationales)*100:.1f} %) metrics were used because they correlate with human judgment.')\n",
        "print(f'{empty} ({empty/len(rationales)*100:.1f} %) metrics had no rationale for being used.')\n",
        "print(f'{len(other)} ({len(other)/len(rationales)*100:.1f} %) metrics provided a rationale (other than following previous work or previously shown correlation with human judgment).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAHe6rZkDNZK"
      },
      "outputs": [],
      "source": [
        "len(rationales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0Kh4_AHQFzd"
      },
      "outputs": [],
      "source": [
        "auto_df['RATB'] = v_bin\n",
        "auto_df['RAT'] = vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-vnhE7wQQvj"
      },
      "outputs": [],
      "source": [
        "grouped_m = auto_df.groupby(by=ID)[METRIC_NAME].agg(lambda x: len(list(x))).to_frame()\n",
        "grouped_r = auto_df.groupby(by=ID)['RAT'].agg(lambda x: list(x)).to_frame()\n",
        "grouped_rb = auto_df.groupby(by=ID)['RATB'].agg(lambda x: 1 if sum(x) >=1 else 0).to_frame()\n",
        "\n",
        "grouped = grouped_m.join(grouped_r)\n",
        "groupedrb = grouped_m.join(grouped_r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy7oTwsARpdV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Create a binary matrix for RAT values\n",
        "all_rat_values = ['None', 'Correlation', 'Following', 'Quality']\n",
        "for rat in all_rat_values:\n",
        "    grouped[rat] = grouped['RAT'].apply(lambda x: x.count(rat) / len(x))\n",
        "\n",
        "# Aggregate counts of metrics and RAT values\n",
        "df_agg = grouped.groupby('Metric name')[all_rat_values].sum().reset_index()\n",
        "\n",
        "# Plot the heatmap\n",
        "df_melted = df_agg.melt(id_vars=['Metric name'], value_vars=all_rat_values,\n",
        "                        var_name='Rationale', value_name='Count')\n",
        "\n",
        "heatmap_data = df_melted.pivot(index=\"Metric name\", columns=\"Rationale\", values=\"Count\")\n",
        "plt.figure(figsize=(7, 7))\n",
        "palette = [\"#fbf2c4\",\"#b8cdab\",\"#74a892\",\"#008585\", \"#4c9eb3\"]\n",
        "cmap = mcolors.LinearSegmentedColormap.from_list(\"n\", palette)\n",
        "ax = sns.heatmap(heatmap_data, annot=True, cmap=cmap)\n",
        "ax.set(xlabel=\"What rationale was given for using a metric?\", ylabel=\"How many metrics were used within one paper?\")\n",
        "#plt.title('Heatmap of Metric Counts vs Rationales given')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trainableness"
      ],
      "metadata": {
        "id": "jmyDeWDnU_NG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRnx8Dne7NyI"
      },
      "outputs": [],
      "source": [
        "inlg = auto_df[auto_df[CONF] == \"INLG 2023\"] #.drop_duplicates(subset=[ID, FAMILY])\n",
        "inlg_train_counts = inlg[TRAIN].value_counts()\n",
        "\n",
        "\n",
        "acl = auto_df[auto_df[CONF] == \"ACL 2023\"] #.drop_duplicates(subset=[ID, FAMILY])\n",
        "acl_train_counts = acl[TRAIN].value_counts()\n",
        "\n",
        "\n",
        "\n",
        "comparison = pd.DataFrame({'INLG 2023': inlg_train_counts, 'ACL 2023': acl_train_counts}).fillna(0).T\n",
        "comparison_normalized = comparison.div(comparison.sum(axis=1), axis=0) * 100\n",
        "ax = comparison_normalized.plot(kind='bar', stacked=True, ylim=(0,100))\n",
        "\n",
        "labels = []\n",
        "\n",
        "plt.ylabel('Percentage')\n",
        "\n",
        "\n",
        "#plt.title('Relative Makeup of Correlation with Human Evaluation', loc='center')\n",
        "plt.legend(title='Is the metric trainable?', bbox_to_anchor=(0.5, -0.1), loc='upper center', ncols=2)\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Annotate percentages on the bars\n",
        "for p in ax.patches:\n",
        "    width, height = p.get_width(), p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "    if height > 5:\n",
        "      ax.text(x + width / 2, y + height / 2, f'{height:.1f}%', ha='center', va='center')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Source v reference based metrics"
      ],
      "metadata": {
        "id": "D0IPR1OxVGHo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UD1idrnEYxa"
      },
      "outputs": [],
      "source": [
        "auto_df[SRC_REF] = auto_df[SRC] + auto_df[REF]\n",
        "replacements = {\n",
        "    \"FALSETRUE\": \"Reference-based metric\",\n",
        "    \"TRUEFALSE\": \"Source-based metric\",\n",
        "    \"TRUETRUE\": \"Metric uses both source and reference\",\n",
        "    \"FALSEFALSE\": \"Metric uses output only\"\n",
        "}\n",
        "auto_df[SRC_REF] = auto_df[SRC_REF].map(replacements)\n",
        "auto_df[SRC_REF].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AIhHo8nEWwt"
      },
      "outputs": [],
      "source": [
        "inlg = auto_df[auto_df[CONF] == \"INLG 2023\"] #.drop_duplicates(subset=[ID, FAMILY])\n",
        "inlg_srcref_counts = inlg[SRC_REF].value_counts()\n",
        "\n",
        "\n",
        "acl = auto_df[auto_df[CONF] == \"ACL 2023\"] #.drop_duplicates(subset=[ID, FAMILY])\n",
        "acl_srcref_counts = acl[SRC_REF].value_counts()\n",
        "\n",
        "palette = sns.color_palette([\"#c7522a\",\"#e5c185\",\"#fbf2c4\",\"#b8cdab\",\"#74a892\",\"#008585\",\"#4c9eb3\",\"#779af5\",\"#a59cff\",\"#dbcdf0\"], n_colors=4)\n",
        "\n",
        "comparison = pd.DataFrame({'INLG 2023': inlg_srcref_counts, 'ACL 2023': acl_srcref_counts}).fillna(0).T\n",
        "comparison_normalized = comparison.div(comparison.sum(axis=1), axis=0) * 100\n",
        "ax = comparison_normalized.plot(kind='bar', stacked=True, color=palette, ylim=(0,100))\n",
        "\n",
        "labels = []\n",
        "\n",
        "plt.ylabel('Percentage')\n",
        "\n",
        "\n",
        "#plt.title('Relative Makeup of Correlation with Human Evaluation', loc='center')\n",
        "plt.legend(title='Type of Metric Used', bbox_to_anchor=(0.5, -0.08), loc='upper center', ncols=2)\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Annotate percentages on the bars\n",
        "for p in ax.patches:\n",
        "    width, height = p.get_width(), p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "    if height > 5:\n",
        "      ax.text(x + width / 2, y + height / 2, f'{height:.1f}%', ha='center', va='center')\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metric split by tasks"
      ],
      "metadata": {
        "id": "xENFBChDH3KO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMLYMddiKjip"
      },
      "outputs": [],
      "source": [
        "#Figure 5 aka \\label{fig:metric_task_usage}\n",
        "MF2 = \"MF2\"\n",
        "auto_tasks = auto_df.copy(deep=True)\n",
        "auto_tasks[TASK] = auto_tasks[TASK].apply(lambda x: list(x))\n",
        "auto_tasks = auto_tasks.explode(TASK)\n",
        "auto_tasks = auto_tasks[~auto_tasks[TASK].isin(['multiple (list all)', 'other (please specify)'])]\n",
        "\n",
        "task_plot_map = {\n",
        "    'question answering': 'question\\nanswering',\n",
        "    'machine translation': 'machine\\ntranslation',\n",
        "    'question generation': 'question\\ngeneration',\n",
        "    'paraphrasing / lossless simplification': 'paraphrasing',\n",
        "    'feature-controlled generation': 'feat-controlled\\ngeneration',\n",
        "    'end-to-end text generation': 'end-to-end\\ngeneration',\n",
        "    'dialogue turn generation': 'dialogue turn\\ngeneration',\n",
        "    'summarisation (text-to-text)': 'summarisation',\n",
        "    'data-to-text generation': 'data-to-text\\ngeneration',\n",
        "    'story generation': 'story\\ngeneration'\n",
        "}\n",
        "plotting_data = dict()\n",
        "task_counts = auto_tasks.drop_duplicates(subset=[ID, TASK])[TASK].value_counts()\n",
        "\n",
        "\n",
        "for t in reversed(task_counts.index):\n",
        "  if task_counts[t] < 6:\n",
        "    continue\n",
        "  largest = auto_tasks[auto_tasks[TASK] == t][MF2].value_counts().head(4).index.tolist()\n",
        "  large = auto_tasks.assign(MF2 = np.where(auto_tasks[MF2].isin(largest), auto_tasks[MF2], 'Other'))\n",
        "  plotting_data[task_plot_map[t]] = large[large[TASK] == t][MF2].value_counts()\n",
        "\n",
        "comparison = pd.DataFrame(plotting_data).fillna(0).T\n",
        "comparison_normalized = comparison.div(comparison.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Sort the values within each task and move 'Other' to the last position\n",
        "def sort_and_move_other(df):\n",
        "    sorted_rows = []\n",
        "    for idx, row in df.iterrows():\n",
        "        sorted_row = row.sort_values(ascending=False)\n",
        "        if 'Other' in sorted_row.index:\n",
        "            other_value = sorted_row.pop('Other')\n",
        "            sorted_row['Other'] = other_value\n",
        "        sorted_rows.append(sorted_row)\n",
        "    return pd.DataFrame(sorted_rows, index=df.index)\n",
        "\n",
        "comparison_normalized = sort_and_move_other(comparison_normalized)\n",
        "\n",
        "# Prepare the data for plotting\n",
        "comparison_normalized = comparison_normalized.reset_index().rename(columns={'index': 'Task'})\n",
        "\n",
        "# Set up the color palette\n",
        "palette = sns.color_palette([\"#c7522a\",\"#e5c185\",\"#fbf2c4\",\"#b8cdab\",\"#74a892\",\"#008585\",\"#4c9eb3\",\"#779af5\",\"#a59cff\",\"#dbcdf0\"], n_colors=comparison_normalized.shape[1] - 1)\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(9, 7))\n",
        "\n",
        "# Function to plot sorted stacked bars\n",
        "def plot_sorted_bars(ax, df, palette):\n",
        "    bottoms = np.zeros(len(df))\n",
        "    for i, column in enumerate(df.columns[1:]):\n",
        "        color = palette[i] if column != 'Other' else 'lightgray'\n",
        "        ax.barh(\n",
        "            df['Task'], df[column],\n",
        "            left=bottoms, label=column, color=color\n",
        "        )\n",
        "        bottoms += df[column]\n",
        "\n",
        "plot_sorted_bars(ax, comparison_normalized, palette)\n",
        "\n",
        "# Add annotations to the bars\n",
        "for i, row in comparison_normalized.iterrows():\n",
        "    cumulative_percentage = 0\n",
        "    for j, (column, value) in enumerate(row.items()):\n",
        "        if column == 'Task':\n",
        "            continue\n",
        "        if value > 5.5:\n",
        "            ax.text(\n",
        "                cumulative_percentage + value / 2,\n",
        "                i,\n",
        "                f'{value:.1f}%',\n",
        "                ha='center', va='center',\n",
        "                fontsize=10, color='black'\n",
        "            )\n",
        "        elif value > 2:\n",
        "            ax.text(\n",
        "                cumulative_percentage + value / 2,\n",
        "                i,\n",
        "                f'{value:.1f}%',\n",
        "                ha='center', va='center',\n",
        "                fontsize=8, color='black'\n",
        "            )\n",
        "        cumulative_percentage += value\n",
        "\n",
        "ax.set_xlabel('Percentage')\n",
        "ax.set_xlim([0, 100])\n",
        "ax.legend(title='Metric', bbox_to_anchor=(0.43, -0.12), loc='upper center', borderaxespad=0., ncols=5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The new version with absolute values and minimal vertical space to be display over the page width\n",
        "# Figure 5 aka \\label{fig:metric_task_usage}\n",
        "\n",
        "# Calculate absolute counts\n",
        "comparison_counts = comparison.T\n",
        "\n",
        "# Function to plot sorted stacked bars with absolute counts and percentages\n",
        "def plot_sorted_bars(ax, df, counts_df, palette):\n",
        "    bottoms = np.zeros(len(df))\n",
        "    for i, column in enumerate(df.columns[1:]):\n",
        "        color = palette[i] if column != 'Other' else 'lightgray'\n",
        "        ax.barh(\n",
        "            df['Task'], df[column],\n",
        "            left=bottoms, label=column, color=color\n",
        "        )\n",
        "        bottoms += df[column]\n",
        "\n",
        "    # Add annotations to the bars\n",
        "    for i, row in df.iterrows():\n",
        "        cumulative_percentage = 0\n",
        "        for j, (column, value) in enumerate(row.items()):\n",
        "            if column == 'Task':\n",
        "                continue\n",
        "            count_value = counts_df.loc[column, row['Task']]\n",
        "            if value > 5.5:\n",
        "                ax.text(\n",
        "                    cumulative_percentage + value / 2,\n",
        "                    i,\n",
        "                    f'{value:.1f}%\\n{int(count_value)}',\n",
        "                    ha='center', va='center',\n",
        "                    fontsize=10, color='black'\n",
        "                )\n",
        "            elif value > 2:\n",
        "                ax.text(\n",
        "                    cumulative_percentage + value / 2,\n",
        "                    i,\n",
        "                    f'{value:.1f}%\\n{int(count_value)}',\n",
        "                    ha='center', va='center',\n",
        "                    fontsize=8, color='black'\n",
        "                )\n",
        "            cumulative_percentage += value\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "plot_sorted_bars(ax, comparison_normalized, comparison_counts, palette)\n",
        "\n",
        "\n",
        "# Formatter function to add percent signs\n",
        "def percent_formatter(x, pos):\n",
        "    return f'{int(x)}%'\n",
        "\n",
        "ax.set_xlim([0, 100])\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "ax.xaxis.set_major_formatter(FuncFormatter(percent_formatter))  # Apply the percent formatter\n",
        "\n",
        "\n",
        "def modify_labels(labels):  # Modify labels to include newline character\n",
        "    return [label.replace(' ', '\\n') for label in labels]\n",
        "\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "labels = modify_labels(labels)\n",
        "ax.legend(handles, labels, title='Metric', loc='center right', bbox_to_anchor=(0.8, 0.3, 0.4, 0.4), )\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vm5aW4cHHvOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Heatmaps"
      ],
      "metadata": {
        "id": "JHBHLnISIKzY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i34GKlrBja8T"
      },
      "outputs": [],
      "source": [
        "# To analyze the number of metrics used per paper\n",
        "acl = auto_df[auto_df[CONF] == \"ACL 2023\"]\n",
        "inlg = auto_df[auto_df[CONF] == \"INLG 2023\"]\n",
        "\n",
        "ag = acl.groupby(by=ID)[METRIC_NAME].apply(lambda x: len(list(x)))\n",
        "ig = inlg.groupby(by=ID)[METRIC_NAME].apply(lambda x: len(list(x)))\n",
        "\n",
        "ig_vals = ig.value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4fWejk98Epq"
      },
      "outputs": [],
      "source": [
        "ag_vals = ag.value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOigD50s9QBR"
      },
      "outputs": [],
      "source": [
        "grouped = auto_df.groupby(by=ID)[FAMILY].agg((lambda x: set(x)))\n",
        "\n",
        "df_agg = grouped.reset_index()\n",
        "\n",
        "# Transform sets to binary matrix\n",
        "all_items = sorted(set().union(*df_agg[FAMILY]))\n",
        "binary_matrix = df_agg[FAMILY].apply(lambda x: [1 if item in x else 0 for item in all_items])\n",
        "binary_df = pd.DataFrame(binary_matrix.tolist(), columns=all_items)\n",
        "\n",
        "# Calculate co-occurrence matrix\n",
        "co_occurrence_matrix = np.dot(binary_df.T, binary_df)\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "palette = [\"#fbf2c4\",\"#b8cdab\",\"#74a892\",\"#008585\", \"#4c9eb3\"]\n",
        "cmap = mcolors.LinearSegmentedColormap.from_list(\"n\", palette)\n",
        "sns.heatmap(co_occurrence_matrix, annot=True, cmap=cmap, xticklabels=all_items, yticklabels=all_items)\n",
        "plt.title('Co-occurrence Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3ySH4SKE-nO"
      },
      "outputs": [],
      "source": [
        "MF2 = \"MF2\"\n",
        "grouped = auto_df.groupby(by=ID)[MF2].agg((lambda x: set(x)))\n",
        "\n",
        "df_agg = grouped.reset_index()\n",
        "\n",
        "# Transform sets to binary matrix\n",
        "all_items = sorted(set().union(*df_agg[MF2]))\n",
        "binary_matrix = df_agg[MF2].apply(lambda x: [1 if item in x else 0 for item in all_items])\n",
        "binary_df = pd.DataFrame(binary_matrix.tolist(), columns=all_items)\n",
        "\n",
        "# Calculate co-occurrence matrix\n",
        "co_occurrence_matrix = np.dot(binary_df.T, binary_df)\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "palette = [\"#fbf2c4\",\"#b8cdab\",\"#74a892\",\"#008585\", \"#4c9eb3\"]\n",
        "cmap = mcolors.LinearSegmentedColormap.from_list(\"n\", palette)\n",
        "sns.heatmap(co_occurrence_matrix, annot=True, cmap=cmap, xticklabels=all_items, yticklabels=all_items)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaQR7Diy_Jgm"
      },
      "outputs": [],
      "source": [
        "def plot_heatmap(df, col1, col2, yname=None, xname=None, title=None, labels=None):\n",
        "    \"\"\"\n",
        "    Plots a heatmap showing the relationship between two string columns.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The input dataframe.\n",
        "    col1 (str): The name of the first string column.\n",
        "    col2 (str): The name of the second string column.\n",
        "    \"\"\"\n",
        "\n",
        "    if xname is None:\n",
        "      xname = col1\n",
        "    if yname is None:\n",
        "      yname = col2\n",
        "    if not title:\n",
        "      title = f'Heatmap of {xname} vs {yname}'\n",
        "    # Create a contingency table (cross-tabulation)\n",
        "    contingency_table = pd.crosstab(df[col1], df[col2])\n",
        "\n",
        "    # Plot the heatmap\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    palette = [\"#fbf2c4\",\"#b8cdab\",\"#74a892\",\"#008585\", \"#4c9eb3\"]\n",
        "    cmap = mcolors.LinearSegmentedColormap.from_list(\"n\", palette)\n",
        "    ax = sns.heatmap(contingency_table, annot=True, cmap=cmap, fmt='d')\n",
        "    ax.set(xlabel=xname, ylabel=None)\n",
        "    if labels is not None:\n",
        "      ax.set_yticklabels(labels)\n",
        "    plt.xticks()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv8Wki-KAChX"
      },
      "outputs": [],
      "source": [
        "labels=['No Correlation with\\nHuman Evaluation', 'Qualitative Correlation\\nwith Human Evaluation', 'Quantitative Correlation\\nwith Human Evaluation', 'No Human Evaluation']\n",
        "plot_heatmap(auto_df, CORRELATED, 'RAT', 'Correlation with Human Evaluation', 'Rationale', labels=labels)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}