{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwgwMiUccT8g"
   },
   "source": [
    "Import the necessary libraries and authenticate the user to access the Google Sheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23596,
     "status": "ok",
     "timestamp": 1714464169683,
     "user": {
      "displayName": "Patricia Schmidtova",
      "userId": "06572757679288945207"
     },
     "user_tz": -120
    },
    "id": "X6KKSyKqryKJ"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "import gspread\n",
    "from google.auth import default\n",
    "creds, _ = default()\n",
    "\n",
    "\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from nltk.metrics import edit_distance, jaccard_distance, masi_distance\n",
    "\n",
    "gc = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amp4RsFyeYKg"
   },
   "source": [
    "Define functions and set columns as variable names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1714464171277,
     "user": {
      "displayName": "Patricia Schmidtova",
      "userId": "06572757679288945207"
     },
     "user_tz": -120
    },
    "id": "IUpR5Yjx6azo"
   },
   "outputs": [],
   "source": [
    "# List the column names as constants to prevent typos\n",
    "ID = \"ACL Paper ID\"\n",
    "METRIC_NAME = \"Metric name\"\n",
    "NEWLY = \"Newly introduced?\"\n",
    "APPENDIX = \"Appendix\"\n",
    "TASK = \"Updated Task\"\n",
    "TASK_OLD = \"Task\"\n",
    "LINK_TO_METRIC = \"Link to the Metric Paper\"\n",
    "PAPER_LINK = \"Link to the Paper\"\n",
    "CORRELATED = \"Corrleated w/ Human Evaluation?\"\n",
    "ANNOTATOR = \"Annotator\"\n",
    "METRIC_IMPL = \"Metric Implementations\"\n",
    "\n",
    "CLOSED_CLASS_COLUMNS = [NEWLY, APPENDIX, CORRELATED]\n",
    "OPEN_CLASS_COLUMNS = [METRIC_NAME, TASK, LINK_TO_METRIC, PAPER_LINK]\n",
    "# Metric needs to be evaluated separately and then we need to merge it to the paper ID to have a unique key and match corresponding lines\n",
    "COLUMNS_TO_EVALUATE = [NEWLY, APPENDIX, TASK, METRIC_IMPL, PAPER_LINK, CORRELATED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1714464174261,
     "user": {
      "displayName": "Patricia Schmidtova",
      "userId": "06572757679288945207"
     },
     "user_tz": -120
    },
    "id": "dXvAiUGb_IlZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# The following snippet checks for the number of annotations per one paper per annotator and reports discrepancies\n",
    "def print_metric_counts(df):\n",
    "  counts = df.groupby([ID, ANNOTATOR]).size().reset_index(name='count')\n",
    "  agreed = disagreed = 0\n",
    "  for i, group in counts.groupby(ID):\n",
    "      unique_counts = group['count'].nunique()\n",
    "      if unique_counts > 1:\n",
    "          disagreed += 1\n",
    "          annotations_info = []\n",
    "          for annotator, count in zip(group[ANNOTATOR], group['count']):\n",
    "              annotations_info.append(f\"{annotator} reported {count} metrics\")\n",
    "          print(f\"For paper id {i}, {'; '.join(annotations_info)}\")\n",
    "      else:\n",
    "        agreed += 1\n",
    "  print(f'With {agreed} agreements and {disagreed} disagreements, annotators agreed in {100 * agreed / float(agreed + disagreed)}% cases.')\n",
    "\n",
    "# Normalize the metric string\n",
    "def normalize_metric(metric):\n",
    "  metric = re.sub('[- +@]+', '', metric) # Remove spaces and other special symbols that might occur - keeping parentheses deliberately\n",
    "  metric = metric.lower() # Lowercase everything\n",
    "  if metric in metric_mapping.keys():\n",
    "    return metric_mapping[metric]\n",
    "  return metric\n",
    "\n",
    "# Normalize URLs\n",
    "def normalize_urls(url):\n",
    "  return re.sub('(/|\\.pdf)$', '', url)\n",
    "\n",
    "def normalize_task(taskstring):\n",
    "  subtasks = re.split('[:;,]', taskstring)\n",
    "  updated = []\n",
    "  for t in subtasks:\n",
    "    task = t.strip().lower()\n",
    "    if task != \"\":\n",
    "      if task in task_mapping.keys():\n",
    "        task = task_mapping[task]\n",
    "      updated.append(task)\n",
    "\n",
    "  return frozenset(updated)\n",
    "\n",
    "\"\"\"\n",
    "  The functions below were taken from the 20 years repo - iaa utilities and modified\n",
    "\"\"\"\n",
    "def extract_iaa_df_by_column_name(annotation_df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Extract a three-column dataframe with `column_name` items grouped by ANNOTATOR (instead of the source spreadsheet) and ID instead of `key`.\"\"\"\n",
    "    if column_name == TASK:\n",
    "      return annotation_df[[ANNOTATOR, ID, column_name]]\n",
    "    return annotation_df[[ANNOTATOR, ID, column_name]] \\\n",
    "        .groupby([ANNOTATOR, ID])[column_name] \\\n",
    "        .apply(frozenset).reset_index()\n",
    "\n",
    "\n",
    "def extract_records_for_nltk(iaa_df: pd.DataFrame):\n",
    "    \"\"\"The first column in the `to_records()` representation is an index, which we don't need for `nltk`.\"\"\"\n",
    "    return [(b, c, d) for _, b, c, d in iaa_df.to_records()]\n",
    "\n",
    "\n",
    "def pretty_print_iaa_by_column(iaa_by_column_dict, values=(\"alpha_jaccard\", \"alpha_masi\")):\n",
    "    print(f\"column\\t{'  '.join(values)}\")\n",
    "    for column in iaa_by_column_dict:\n",
    "        values_string = '    '.join([f\"{iaa_by_column_dict[column][value]:.2f}\" for value in values])\n",
    "        print(f\"{column}\\t{values_string}\")\n",
    "\n",
    "def run_closed_class_jaccard_and_masi(df: pd.DataFrame, columns):\n",
    "        iaa_by_column = {column: {\"df\": extract_iaa_df_by_column_name(df, column)} for column in columns}\n",
    "\n",
    "        for column in iaa_by_column:\n",
    "            #if column == APPENDIX:\n",
    "            #  print(iaa_by_column[column]['df'].groupby(ID).head())\n",
    "            task = AnnotationTask(distance=jaccard_distance)\n",
    "            task.load_array(extract_records_for_nltk(iaa_by_column[column]['df']))\n",
    "            iaa_by_column[column]['alpha_jaccard'] = task.alpha()\n",
    "            #iaa_by_column[column]['fleissk_jaccard'] = task.multi_kappa()\n",
    "\n",
    "            task = AnnotationTask(distance=masi_distance)\n",
    "            task.load_array(extract_records_for_nltk(iaa_by_column[column]['df']))\n",
    "            iaa_by_column[column]['alpha_masi'] = task.alpha()\n",
    "            #iaa_by_column[column]['fleissk_masi'] = task.multi_kappa()\n",
    "        return iaa_by_column\n",
    "\n",
    "def print_absolute_agreement(dataframe: pd.DataFrame, iaa_by_column_dict=None, columns=[METRIC_NAME]):\n",
    "        if iaa_by_column_dict is None:\n",
    "            iaa_by_column_dict = run_closed_class_jaccard_and_masi(dataframe, columns)\n",
    "            pretty_print_iaa_by_column(iaa_by_column_dict)\n",
    "            print()\n",
    "        for column in columns:\n",
    "            df = iaa_by_column_dict[column]['df']\n",
    "            print(f\"Interannotator agreement for {column}\")\n",
    "            annotator_list = dataframe[ANNOTATOR].unique()\n",
    "            print(\" \\t\" + \"\\t\".join([str(annotator) for annotator in annotator_list]))\n",
    "            for a1 in annotator_list:\n",
    "                a1_vals = list(df[df[ANNOTATOR] == a1][column])\n",
    "                print(f\"{a1}\", end=\"\\t\")\n",
    "                pairwise_agreements = []\n",
    "                for a2 in annotator_list:\n",
    "                    a2_vals = list(df[df[ANNOTATOR] == a2][column])\n",
    "                    agreement_sum = 0\n",
    "                    for a1_val, a2_val in zip(a1_vals, a2_vals):\n",
    "                        agreement_sum += 1 - jaccard_distance(a1_val, a2_val)\n",
    "                    pairwise_agreements.append(agreement_sum / min(len(a1_vals), len(a2_vals)))\n",
    "                    print(f\"{pairwise_agreements[-1]:.2f}\", end=\"\\t\")\n",
    "                print(f\"\\t{(sum(pairwise_agreements) - 1) / (len(pairwise_agreements) - 1):.2f}\")\n",
    "            print()\n",
    "            print()\n",
    "\n",
    "def print_absolute_agreement_by_id(dataframe: pd.DataFrame, iaa_by_column_dict=None, columns=[METRIC_NAME]):\n",
    "        if iaa_by_column_dict is None:\n",
    "            iaa_by_column_dict = run_closed_class_jaccard_and_masi(dataframe, columns)\n",
    "            pretty_print_iaa_by_column(iaa_by_column_dict)\n",
    "            print()\n",
    "        for column in columns:\n",
    "            df = iaa_by_column_dict[column]['df']\n",
    "            print(f\"Interannotator agreement for {column}\")\n",
    "            annotator_list = dataframe[ANNOTATOR].unique()\n",
    "            print(\" \\t\" + \"\\t\".join([str(annotator) for annotator in annotator_list]))\n",
    "            for a1 in annotator_list:\n",
    "                a1_vals = df[df[ANNOTATOR] == a1].set_index(ID)[column].to_dict()\n",
    "                print(f\"{a1}\", end=\"\\t\")\n",
    "                pairwise_agreements = []\n",
    "                for a2 in annotator_list:\n",
    "                    a2_vals = df[df[ANNOTATOR] == a2].set_index(ID)[column].to_dict()\n",
    "                    agreement_sum = 0\n",
    "                    for a1_key in a1_vals.keys():\n",
    "                        if not a1_key in a2_vals.keys():\n",
    "                          continue\n",
    "                        a1_val = a1_vals[a1_key]\n",
    "                        a2_val = a2_vals[a1_key]\n",
    "                        agreement_sum += 1 - jaccard_distance(a1_val, a2_val)\n",
    "                    pairwise_agreements.append(agreement_sum / min(len(a1_vals), len(a2_vals)))\n",
    "                    print(f\"{pairwise_agreements[-1]:.2f}\", end=\"\\t\")\n",
    "                print(f\"\\t{(sum(pairwise_agreements) - 1) / (len(pairwise_agreements) - 1):.2f}\")\n",
    "            print()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2tQssy2eOlc"
   },
   "source": [
    "Open the worksheet and make it into a DataFrame + Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1912,
     "status": "ok",
     "timestamp": 1714464181975,
     "user": {
      "displayName": "Patricia Schmidtova",
      "userId": "06572757679288945207"
     },
     "user_tz": -120
    },
    "id": "OpxEFlrjs3Ih"
   },
   "outputs": [],
   "source": [
    "worksheet = gc.open_by_url('https://docs.google.com/spreadsheets/d/1NU6IlxhYg515RLjsVxNW5FS0ChWbIKi3yrrdfqEvsRM/').worksheet('IAA')\n",
    "\n",
    "# get_all_values gives a list of rows.\n",
    "rows = worksheet.get_all_values()\n",
    "\n",
    "# Convert to a DataFrame and render.\n",
    "df = pd.DataFrame.from_records(rows[1:], columns=rows[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1714464181977,
     "user": {
      "displayName": "Patricia Schmidtova",
      "userId": "06572757679288945207"
     },
     "user_tz": -120
    },
    "id": "lTcwB4QF3nyU"
   },
   "outputs": [],
   "source": [
    "metric_mapping = {\n",
    "    'harmonicmean(hmean)between(1âˆ’pbleu)andbleu': 'harmonicmean(pbleubleu)',\n",
    "    'harmonicmeanof1pbleuandbleu': 'harmonicmean(pbleubleu)',\n",
    "    'hmeanbetween(1pbleu)andbleu':'harmonicmean(pbleubleu)',\n",
    "    'harmonicmeanofbleu4andstyleaccuracy': 'harmonicmean(bleu4styleaccuracy)',\n",
    "    'pairwisebleu': 'pbleu',\n",
    "    'pbleu(selfbleu)': 'pbleu',\n",
    "    'em': 'exactmatch',\n",
    "    'exactmatch(em)': 'exactmatch',\n",
    "    'inform(rate)': 'inform',\n",
    "    'success(rate)': 'success',\n",
    "    'combinescore(informandrate)': 'combinedscore(informandrate)',\n",
    "    'bleu(4)': 'bleu4',\n",
    "    'accuracy(?)': 'accuracy',\n",
    "    'macroaveragedf1score(f1)': 'f1',\n",
    "    'sensitivity': 'demetrbenchmarksensitivityscores',\n",
    "    'bleurtbase': 'bleurt',\n",
    "    'allmpnetbasev2': 'mpnetcosinesimilarity',\n",
    "    'negmpnet': 'negmpnetcosinesimilarity',\n",
    "    'distinct1': 'distinctunigrams',\n",
    "    'distinct2': 'distinctbigrams',\n",
    "    'distinct4': 'distinct4grams'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1714464184870,
     "user": {
      "displayName": "Patricia Schmidtova",
      "userId": "06572757679288945207"
     },
     "user_tz": -120
    },
    "id": "jnod6kamyAtD"
   },
   "outputs": [],
   "source": [
    "task_mapping = {\n",
    "    \"natural language entailment)\": \"natural language inference\",\n",
    "    \"data-text generation\": \"data-to-text generation\",\n",
    "    \"data-to-text\": \"data-to-text generation\",\n",
    "    \"dialogue generation\": \"dialogue turn generation\",\n",
    "    \"dialogue response\": \"dialogue turn generation\",\n",
    "    \"dialouge\": \"dialogue turn generation\",\n",
    "    \"open-ended dialogue\": \"dialogue turn generation\",\n",
    "    \"task-oriented dialouge\": \"dialogue turn generation\",\n",
    "    \"paraphrase generation\": \"paraphrasing / lossless simplification\",\n",
    "    \"text simplification\": \"compression / lossy simplification\",\n",
    "    \"question-generation\": \"question generation\",\n",
    "    \"quora question pairs\": \"question answering\",\n",
    "    \"story-generation\": \"story generation\",\n",
    "    \"summarisation\": \"summarisation (text-to-text)\",\n",
    "    \"summarization\": \"summarisation (text-to-text)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1714464187514,
     "user": {
      "displayName": "Patricia Schmidtova",
      "userId": "06572757679288945207"
     },
     "user_tz": -120
    },
    "id": "ksciZvly8_px"
   },
   "outputs": [],
   "source": [
    "# Normalize\n",
    "df[METRIC_NAME] = df[METRIC_NAME].apply(normalize_metric)\n",
    "df[PAPER_LINK] = df[PAPER_LINK].apply(normalize_urls)\n",
    "df[LINK_TO_METRIC] = df[LINK_TO_METRIC].apply(normalize_urls)\n",
    "df[TASK] = df[TASK].apply(normalize_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1714047555571,
     "user": {
      "displayName": "Patricia Schmidtova",
      "userId": "06572757679288945207"
     },
     "user_tz": -120
    },
    "id": "QtSDqyKo4EGn",
    "outputId": "164ad8b0-f6d9-4779-8239-0b3290556be3"
   },
   "outputs": [],
   "source": [
    "uni = set ()\n",
    "for fs in df[TASK].unique():\n",
    "  uni.update(fs)\n",
    "uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kumLJADnFt9A"
   },
   "source": [
    "Calculate IAA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1714464192212,
     "user": {
      "displayName": "Patricia Schmidtova",
      "userId": "06572757679288945207"
     },
     "user_tz": -120
    },
    "id": "AXrcGL4aYKLJ",
    "outputId": "65ed0629-fa95-4305-b1c0-d3abf7508aac"
   },
   "outputs": [],
   "source": [
    "# Quick glance at the number of metrics found by each annotator\n",
    "print_metric_counts(df)\n",
    "print()\n",
    "\n",
    "# Make sure everyone has the same ordering of papers:\n",
    "df = df.sort_values(by=ID)\n",
    "\n",
    "# Print agreement on all metrics including human\n",
    "print('IAA including human metrics:')\n",
    "print_absolute_agreement(df)\n",
    "\n",
    "# Watch out, maybe there is a paper with only human metrics\n",
    "\n",
    "# Now exclude human metrics\n",
    "df_automatic = df[df[METRIC_NAME].str.contains('human|n/a') == False]\n",
    "print('IAA after excluding human metrics:')\n",
    "print_absolute_agreement(df_automatic)\n",
    "\n",
    "# Create a copy of the dataframe to calculate IAA for ID + metric pairs\n",
    "df_by_metrics = df_automatic.copy(deep=True)\n",
    "df_by_metrics[ID] = df_by_metrics[ID] + '-' + df_by_metrics[METRIC_NAME]\n",
    "\n",
    "\n",
    "# For each paper+metric, compute the agreement for the remaining columns\n",
    "print_absolute_agreement_by_id(df_by_metrics, None, COLUMNS_TO_EVALUATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1713174362275,
     "user": {
      "displayName": "Patricia Schmidtova",
      "userId": "06572757679288945207"
     },
     "user_tz": -120
    },
    "id": "wuOPr59o3md9",
    "outputId": "393a7987-ffcf-4ea8-fcda-90c0458cf2a6"
   },
   "outputs": [],
   "source": [
    "df_automatic[METRIC_NAME].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 368,
     "status": "ok",
     "timestamp": 1713174378220,
     "user": {
      "displayName": "Patricia Schmidtova",
      "userId": "06572757679288945207"
     },
     "user_tz": -120
    },
    "id": "e1HC2VpVJmFW",
    "outputId": "7101c600-aa08-469f-de29-6c91d29fb9ab"
   },
   "outputs": [],
   "source": [
    "df_automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tTlIZd_JnNu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNiEX/Rqw9saeU5Vgd1S1rI",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
